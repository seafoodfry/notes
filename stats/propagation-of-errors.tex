%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dynamic Properties of the Monte Carlo Method in Statistical Physics}

These are notes from the paper
"Dynamic Properties of the Monte Carlo Method in Statistical Physics"
by H. Muller-Krumbhaar and K Binder,
\href{https://link.springer.com/article/10.1007/BF01008440}{Journal of Statistical Physics, Vol 8, No. 1, 1973}.

References 5, 6, 27, and 28 mention processes for estimating statistical errors.

However, the processes mentioned in ref 28 disregard the knowledge that one can draw from
transition probabilities about the well-defined correlations between subsequent configurations.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Central Limit Theorem}

A collection of random variables is independent and identically distributed (IID)
if each random variable has the same probability distribution as the others and
all are mutually independent.

Another way of thinking about an IID set of random variables is as a random sample.

If an arbitrarily large number of samples, each involving multiple observations (data points),
were separately used in order to compute one value of a statistic
(such as, for example, the sample mean or sample variance) for each sample,
then the sampling distribution is the probability distribution of the values that the statistic takes on.
\\

The central limit theorem establishes that, in many situations,
for independent and identically distributed random variables,
the sampling distribution of the standardized sample mean tends towards the standard normal distribution
even if the original variables themselves are not normally distributed.
\\

Normal distributions are often used to represent real-valued random variables
whose distributions are not known because of the central limit theorem.
One example of IID data could be measurement errors.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussians and the Error Function}

The error function was first formally introduced in
\href{https://books.google.com/books?id=8Po7AQAAMAAJ&pg=RA1-PA294#v=onepage&q&f=false}{On a class of definite integrals by J.W.L. Glaisher B.A. F.R.A.S. F.C.P.S.}.
(We found that link in the wikipedia page talking about the Error function
\href{https://en.wikipedia.org/wiki/Error_function}{Error function}, it was reference number 3.)

Some great references we recommend watching and working through are
\begin{enumerate}
\item \href{https://www.youtube.com/watch?v=jkytxdedxhU}{The impossible integral of e\^ (x\^ 2) \& the error function by blackpenredpen}
\item \href{https://www.youtube.com/watch?v=zorcLisjRUI}{Integral of e\^(x\^2) \& the Imaginary Error Function by blackpenredpen}
\end{enumerate}

$$
\erf(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x} e^{-t^2} dt
$$

This is the definite integral version of a Gaussian integral.
Since $\int_{-\infty}^{\infty} x^{-x^2} dx = \sqrt{\pi}$ and since the definite integral we use to define the
error function only goes from 0 to $x$ (only half the domain of the Gaussian),
that's why we normalize the $\erf(x)$ with a $\sqrt{\pi}/2$.
\\

To draw the horizontal asymptotes of $\erf(x)$, note that the domain is $\left(-\infty, \infty\right)$, so we need to take the
limits of $x$ as it tends to positive and negative infinity.

\begin{align*}
\lim_{x\rightarrow -\infty} \erf(x) &= \frac{2}{\sqrt{\pi}} \int_{0}^{-\infty} e^{-t^2} dt \\
&= - \frac{2}{\sqrt{\pi}} \int_{-\infty}^{0} e^{-t^2} dt \\
&= -1
\end{align*}

Similarly, $\lim_{x\rightarrow -\infty} \erf(x) = 1$.
\\

The derivative is also quite easy to get since we are taking the derivative of an integral and thus the fundamental theorem of calculus
is all we need,
$$
\erf^{\prime} (x) = \frac{2}{\sqrt{\pi}} e^{-x^2}
$$
From the derivative we can also tell that $\erf(x)$ has no critical numbers and as a matter of fact, the derivative is always non-negative.

And since we are talking about critical points, we can also go and take a look at the inflection points by computing the second
derivative,
$$
\erf^{\prime\prime}(x) = -2x \frac{2}{\sqrt{\pi}} e^{-x^2}
$$
From this expression we can tell that $x=0$ is an inflection point, and that for all $x<0$ $\erf(x)$ is concave down, and for $x>0$
$\erf(x)$ is concave up.
\\

Now, what about $\int e^{x^2} dx$, here we could do a variable transformation,
\begin{align*}
\int e^{x^2} dx &= \int e^{-\left(-x^2\right)} dx \\
& = \int e^{-(ix)^2} dx \rightarrow
\left[
    \begin{alignedat}{2}
        u  &= ix    \quad & x  &= \frac{1}{i}u = \frac{1}{i} \frac{i}{i} u = -iu \\
        du &= i dx  \quad & dx &= -i du
    \end{alignedat}\,
\right] \\
&= \int e^{-(u)^2} (-i) du \\
&= -i \int e^{-u^2} du \\
&= -i \frac{2}{\sqrt{\pi}} \erf(u) \\
&= -i \frac{2}{\sqrt{\pi}} \erf(ix) = \erfi(x)
\end{align*}

There is one more property to mention before we continue looking into variable transformations and that is that
$\erf(-z) = -\erf(z)$, the error function is an odd function.
This directly results from the fact that the integrand $e^{-t^2}$ is an even function
(the antiderivative of an even function which is zero at the origin is an odd function and vice versa).