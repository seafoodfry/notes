\chapter{Integrals in QFT}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussians}

The simples one,
$$
\int e^{-x^2} \, dx = \sqrt{\pi}
$$

The most commn trick seems to have been popularized by Poisson, see
\href{https://en.wikipedia.org/wiki/Gaussian_integral}{wiki: Gaussian integral}:

\begin{align*}
\left( \int dx \, e^{-x^2} \right)^2 &= \left( \int dx \, e^{-x^2} \right) \left( \int dy \, e^{-y^2} \right) \\
&= \iint e^{-\left(x^2 + y^2\right)} \, dx dy
\end{align*}
Going to polar coordinates, where $r^2 = x^2 + y^2$ and the $dx \, dy$ measure becomes $r dr d\theta$, we have

\begin{align*}
\iint_{\mathbb{R}^2} e^{-\left(x^2 + y^2\right)} \, dx dy &=
    \int_{0}^{2\pi} \int_{0}^{\infty} e^{-r^2} r dr d\theta \\
&= 2\pi \int_{0}^{\infty} e^{-r^2} r dr
\end{align*}

The u-substitution of $u = -r^2$ makes $du = -2r dr$ or $r dr = -\frac{1}{2} du$.
With this substitution we also need to evaluate the limits of integration.
When $r \rightarrow 0$, $u = -r^2 \rightarrow 0$.
And when $r \rightarrow \infty$, $u = -r^2 \rightarrow -\infty$.
So we have,

\begin{align*}
2\pi \int_{0}^{-\infty} e^{-r^2} r dr &= -\pi \int_{0}^{\infty} e^{u} du \\
&= -\pi \left( \lim_{x\rightarrow \infty} e^{-\infty} - e^0 \right) \\
&= -\pi \left( 0 - 1  \right) \\
&= \pi
\end{align*}

And thus we have our proof.

If we instead had landed on:
$$
\int e^{-ar^2} r dr
$$

Then our u-substitution would have looked as $u = -ar^2$
and $du = -2ar dr$, so $r dr = -\frac{1}{2a}$.
The limits of integration would have looked the same but we would have ended up with a factor of $a$
in the denominator giving us another handy Gaussian.

$$
\int e^{-ax^2} dx = \sqrt{ \frac{\pi}{a} }
$$

And if we had had
$$
\int e^{-a\left(x + b\right)^2} dx
$$

The trick is to do the u-substitution earlier.
We now have $u = x+b$ so that $du = dx$, the limits of integration don't change and we'd have
$$
\int e^{-a\left(x + b\right)^2} dx
=
\int e^{-a u^2} dx
$$
Which we already know the answer to, and $b$ seems to not matter at all!

And finally, but very importantly, we will show how to complete the square in order to solve

$$
\int e^{-ax^2 + bx + c} dx
$$

To complete the square, we want to get $ax^2 + bx + c$ into the form $a(x+b) + k$ so we can
re-use the previous result.
\begin{enumerate}
\item factor out $a$: $a \left(x^2 + \frac{b}{a}x \right) + c$
\item take the coefficient of $x$, divide it by $2$ and square it: $a \left(x^2 + \frac{b}{a}x + \left(\frac{b}{2a}\right)^2 - \left(\frac{b}{2a}\right)^2 \right) + c$
\item by this point we have a perfect square, so separate it: $a \left(x + \frac{b}{a} \right)^2 - a\left(\frac{b}{2a}\right)^2 + c$
\end{enumerate}

With this trcik we can arrive at

$$
\int e^{-\left(ax^2 + bx + c\right)} dx = \sqrt{\frac{\pi}{a}} e^{\frac{b^2}{4a} - c}
$$

But remember how to complete the square!
We will be using it more later on.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Complex Gaussians: the Fresnel Integral} \label{integrals:complex-gaussian}


Convergence of $\int e^{-at^2} dt$.

When $a \in \mathbb{R}$ and $a>0$, $e^{-at^2}$ decays rapidly as $|t| \rightarrow \infty$.

When $a = re^{i\theta}$ we can write the phase factor as
$$
e^{-at^2} = e^{-rt^2 \left(\cos\theta + i\sin\theta\right)}
= e^{-rt^2 \cos\theta}
    e^{-irt^2 \sin\theta}
$$

If we remember Euler's formula, we'll see that the imaginary component just spins around.
So convergence is solely controlled by the real part.
For the real part to converge, we need $-rt^2 \cos\theta$ to stay negative as $|t| \rightarrow \infty$.
For this to happen, we need to keep $\theta \in \left(-\pi/2, \pi/2\right)$.

With this in mind, we can perform an analytic continuation by rotating $\theta$ from $0$ to $\pi/2$.
This is because both $\int e^{-at^2} dt$ and its real-solution is $\sqrt{\frac{\pi}{a}}$ are both analytic
and we keep the integral from exploding along the path we chose.
A fundamental theorem of complex analysis, the
\href{https://en.wikipedia.org/wiki/Identity_theorem}{identity theorem} says that
if two analytic functions are equal on a line segment and both are analytic in a larger region
then they must be equal throughout that region.

So for example, when we have $\int e^{-\frac{1}{2} it^2} dt$, then
$a = r e^{i\theta} = \left(\frac{1}{2}\right)\left(-i\right)$.
Thus,
$$
\int e^{-\frac{1}{2} it^2} dt = \sqrt{ \frac{\pi}{\left(\frac{1}{2}\right)\left(-i\right)} }
= \sqrt{2\pi} \left(\sqrt{-\frac{1}{i} }\right)
= \sqrt{2\pi} \sqrt{i}
= \sqrt{2\pi} e^{i\pi/4}
$$



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Kinetic operator and a Green Function} \label{integral:kinetic-operator}

This problem is part of
\href{https://arxiv.org/abs/physics/0609191}{MontePython: Implementing Quantum Monte Carlo using Python}.

There is a kinetic operator defined as
$$
G_K \left(\mathbf{R}^\prime, \mathbf{R}, t\right) =
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-i\mathbf{k}\mathbf{R}^\prime} e^{-Dtk^2} e^{-i\mathbf{k}\mathbf{R}} d\mathbf{k}
$$

This integral then is integrated and the following Green function is obtained
$$
G_K \left(\mathbf{R}^\prime, \mathbf{R}, t\right) =
\frac{1}{\left(4\pi Dt\right)^{3N/2}} e^{-\left(\mathbf{R}-\mathbf{R}^\prime\right) / 4Dt}
$$

And as we saw in \hyperref[jackson:problem-1.2]{Jackson problem 1.2}, the Green function we just got is equivalent to
$\delta\left(\mathbf{R} - \mathbf{R}^\prime\right)$.

But coming back to the integral, the trick is to complete the square!
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-i\vect{k}\vect{R}^\prime} e^{-Dtk^2} e^{-i\vect{k}\vect{R}} d\vect{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dtk^2 + -i\vect{k}\cdot\left(\vect{R}-\vect{R}^\prime\right)} d\mathbf{k}
\end{align*}

This last integral has the form $\int e^{ax^2 + bx} dx$, which can be solved as follows,
\begin{align*}
\int e^{ax^2 + bx} dx &= \int \exp\left\{ a\left(x^2 + \frac{b}{a}x\right) \right\} dx \\
&= \int \exp\left\{ a\left(x^2 + \frac{b}{a}x + \left(\frac{b}{2a}\right)^2 - \left(\frac{b}{2a}\right)^2 \right) \right\} dx \\
&= \int \exp\left\{ a\left(x + \frac{b}{2a}\right)^2 - \frac{b^2}{4a} \right\} dx \\
&= e^{-b^2/4a} \int e^{ a\left(x + \frac{b}{2a}\right)^2 } dx
\end{align*}

Now, let's look at $\int e^{ a\left(x + b/2a\right)^2 } dx$.
First, we need to make a transformation.
Let's define $u = x + b/2a$, then $du = dx$, the measure remains invariant (so do the limits of integration), so
$\int e^{ a\left(x + b/2a\right)^2 } dx = \int e^{ au^2 } du$.
Now, if we define $a = -c$, then 

\begin{align*}
\int e^{ a\left(x + b/2a\right)^2 } dx &= \int e^{ au^2 } du \\
&= \int e^{ -cu^2 } du \\
&= \sqrt{ \frac{\pi}{c} } = \sqrt{ \frac{\pi}{-a} } 
\end{align*}

This whole thing works if $a < 0$ but it turns out that this is also valid if $\re{(a)} \leq 0$ but $a \neq 0$.

Putting everything back together,
$$
\int e^{ax^2 + bx} dx = \sqrt{ \frac{\pi}{-a} } e^{-b^2/4a}
$$

Looking back at our original problem, we can chose $a = -Dt$ and $b = i \vect{r}$, where $\vect{r} = \vect{R}-\vect{R}^\prime$.
and so
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dtk^2 + -i\vect{k}\cdot\vect{r}} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k^2 + \frac{i\vect{k}}{Dt}\cdot\vect{r} \right)} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k^2 + \frac{i\vect{r}}{Dt}\cdot\vect{k} + \left(\frac{i\vect{r}}{2Dt}\right)^2 - \left(\frac{i\vect{r}}{2Dt}\right)^2 \right)} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 - \frac{ \vect{r}^2 }{ 4Dt } } d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } d\mathbf{k} \\
\end{align*}

Again, since $\int e^{-ax^2} dx = \sqrt{ \frac{\pi}{a} }$,
then
\begin{align*}
\int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } dk &= \sqrt{ \frac{\pi}{Dt} }
\end{align*}

Thus,
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \left( \sqrt{ \frac{\pi}{Dt} } \right)^{3N} \\
&= \frac{1}{\left(4\pi^2\right)^{3N/2}} \frac{\pi^{3N/2}}{(Dt)^{3N/2}} e^{-\vect{r}^2 /4Dt } \\
&= \frac{1}{\left(4\pi Dt\right)^{3N/2}} e^{-\left(\vect{R} - \vect{R}^\prime\right)^2 /4Dt }
\end{align*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Klein-gordon Non-relativistic Amplitude} \label{ps-kg:nonrelativistic-propagator}

The following integral comes from Peskin and schroeder Section 2.1
\begin{align}
\int \frac{d^3p}{(2\pi)^3} \, e^{-i(\vect{p}^2/2m)t} e^{i\vect{p}\cdot(\vect{x}-\vect{x}_0)}
&= \left( \frac{m}{2\pi i t} \right)^{3/2} e^{im (\vect{x}-\vect{x}_0)^2 /2t}
\end{align}

This integral follows the same procedure as \ref{integral:kinetic-operator}.

Here we have $a = -\frac{it}{2m}$ and $b = i(\vect{x}-\vect{x_0})$.
Since $\re{a} = -t/2m \leq 0$ and $a\neq 0$ when $t\neq 0$ our solution is valid for $t\neq 0$.
Then

$$
\sqrt{ \frac{\pi}{-a} } = \sqrt{ \frac{2\pi m}{it} }
$$

and
\begin{align*}
-\frac{b^2}{4a} &= \frac{2m \left(\vect{x}-\vect{x}_0\right)^2}{-4it} \\
&= \frac{im \left(\vect{x}-\vect{x}_0\right)^2}{2t}
\end{align*}

So the solution is
\begin{align*}
& \frac{1}{(2\pi)^3} \left( \frac{2\pi m}{it} \right)^{3/2} e^{\frac{im \left(\vect{x}-\vect{x}_0\right)^2}{2t}} \\
&= \left( \frac{m}{2\pi it} \right)^{3/2} e^{im \left(\vect{x}-\vect{x}_0\right)^2 /2t}
\end{align*}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Contour Integrals}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poles}

A \text{pole} is a type of singularity.

Some complex analysis to put it into perspective, the following comes from
\url{https://en.wikipedia.org/wiki/Zeros_and_poles}:
\begin{itemize}
\item A function of a complex variable $z$ is \textbf{holomorphic} in an open domain $U$ if it is
    differentiable with respect to $z$ at every point in $U$. Another equivalent definition is that A
    function is holomorphic if it is analytic, that is, if its Taylor series exists at every point in $U$
    and converges to the function in some neighborhood of the point in $U$.
\item A function is \textbf{meromorphic} in $U$ if every point of $U$ has a neighborhood such that at least
    one of $f(z)$ or $1/f(z)$ are holomorphic in it.
\item A \textbf{zero} of a meromorphic function $f$, is a complex number $z$, such that $f(z)=0$.
\item A \textbf{pole} of $f$ is a zero of $1/f(z)$.
\end{itemize}

Let's also introduce some other good results summarized by \cite{gifted-qft} in appendix B.

If $f(z)$ is a holomorphic function, the function can be expanded in terms of its Laurent series around
some point $z_0$ in its open domain as follows:

\begin{align*}
f(z) &= \sum_{n=-\infty}^{\infty} a_n \left(z - z_0\right)^n \\
&= a_0 + a_1 \left(z-z_0\right) + a_2 \left(z-z_0\right)^2 + \dots +
    \frac{b_1}{\left(z-z_0\right)} + \frac{b_2}{\left(z-z_0\right)^2} + \dots
\end{align*}

\begin{itemize}
\item if all $b_n$ are 0, then $f(z=z_0)$ is analytic at $z=z_0$.
\item if all $b_n$ when $n>1$ are zero, then we have a pole of order $n$ at $z=z_0$. When $n=1$,
    we have a simple pole.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Some Theorems}

\textbf{Cauchy's theorem or Cauchy's integral theorem} says the following:
if $f(z)$ is holomorphic on a simpli connected domain $\Omega$, then for any simply close contour $C$
in $\Omega$,

$$
\oint_C f(z) \, dz = 0
$$

This may look weird at first but it is another version of the fundamental theorem of calculus -
if you integrate and go all around some line but end up in the same place then you didn't really go
anywhere.
\\

\textbf{Cauchy's integral formula} says:
if $U$ is an open subset in $\mathcal{C}$, a $D$ is a closed disk of radius $r$ defined as
$$
D = \{ z : |z - z_0| \leq r \}
$$
and $D$ is completely contained in $U$.
If we have a holomorphic function $f : U \rightarrow \mathcal{C}$, and $\gamma$ a circle oriented
counterclockwise forming the boundary for $D$,
then for every $a$ in the interior of $D$

$$
f(a) = \frac{1}{2\pi i} \oint_\gamma \frac{f(z)}{z-a} dz
$$

This formula can be generalized in a couple ways.

The first generalization is \textbf{Cauchy's differentiation formula}:

$$
f^{\left(n\right)}(a) = \frac{n!}{2\pi i} \oint_\gamma \frac{f(z)}{\left(z-a\right)^{n+1}} dz
$$

The second is to obtain the coefficients of the function's Laurent series:

$$
a_n = \frac{1}{2\pi i} \oint_\gamma \frac{f(z)}{\left(z-a\right)^{n+1}} dz
$$

Lastly, you'll need \textbf{Jordan's lemma} which states that:
if $f$ is defined on a semicircular contour

$$
C_{R} = \{ z = Re^{i\theta} | \theta \in [0, \pi] \}
$$

where $R > 0$ and $C_R$ lies in the upper half plane and is centered at the origin.
If $f$ is of the form

$$
f(z) = e^{iaz} g(z)
$$

with $a>0$.
In this case, Jordan's lemma provides the following upper bound for the contour integral:

$$
\left| \int_{C_{R}} f(z) \, dz \right| \leq \frac{\pi}{a} M_R
    = \frac{\pi}{a} \max_{\theta \in [0,\pi]} \left| g \left(Re^{i\theta}\right) \right|
$$

An analogous statement for a semicircular contour in the lower half-plane holds when $a < 0$. 

An alternate definition is as follows:
the value of the integral

$$
\int g(x) e^{iax} dx
$$
along the infinite upper semicircle and with $a>0$ is 0 for "nice" functions which satisfy
$\lim_{R\rightarrow \infty} \left| g\left(Re^{i\theta}\right) \right| = 0$.

Which also holds when the integral in question is $\int g(z) e^{iaz} dz$ when performed along
an infinite upper semicircle.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Residues}

\textbf{Residues} are important because there is 