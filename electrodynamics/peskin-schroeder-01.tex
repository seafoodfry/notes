\chapter{Intro to QFT}

\section{Inivitation}

\subsection{Polarization Vectors} \label{section:polarized-vectors}

In the invitation, we are presented with the polarization vector $\epsilon^\mu = (0, 1, i, 0)$.

To understand this notation let's go through some examples taken from
\href{https://scholar.harvard.edu/files/schwartz/files/lecture14-polarization.pdf}{Lecture 14: Polarization}.

We say that a plane wave is \textbf{linearly polarized} if there is no phase difference between $E_x$ and $E_y$:
$$
\vect{E}_0 = \left( E_x, E_y, 0 \right)
$$

Then a linearly polarized plane wave in the x direction looks like $\vect{E} = E_0 e^{i(kz-wt)} (1, 0, 0)$.
And a linearly polarized plane wave in the y direction looks like $\vect{E} = E_0 e^{i(kz-wt)} (0, 1, 0)$.

The thing to remember is that we are implicitly only looking at the real part, so a linearly polarized wave in the x
direction is actually $(E_0 \cos(kz-wt), 0, 0)$.
\\

\textbf{Circular polarization} is when the electric field components are one-quarter out of phase ($\pi /2$).
Then the field can be written as,

\begin{align*}
\vect{E}_0 &= \left( E, E e^{i\pi/2}, 0 \right) \\
&= \left( E, iE, 0 \right) \\
&= E_0 \left( e^{i(kz-wt)}, ie^{i(kz-wt)}, 0 \right)
\end{align*}

And since we only care about the real parts,
$$
\vect{E}_0 = \left( cos(kz-wt), -sin(kz-wt), 0 \right)
$$

This is interesting because if you jump to the wikipedia page for "List of trigonometric identities"
and look for the "Shift by one quarter period" table, you'll see that
$$
\sin(\theta + \frac{\pi}{2}) = \cos(\theta)
$$
and
$$
\cos(\theta + \frac{\pi}{2}) = -\sin(\theta)
$$

(A shit by a quarter wavelength is essentially differentiation!)

But here is the catch and the connection with P\&S: we can also get the same result if we have
$$
\vect{E} = E_0 \left( \cos(wt-kz)\hat{x} - \sin(wt-kz)\hat{y} \right)
$$
We can also write is as,
$$
E_0 e^{i(wt - kz)} (0, 1, i, 0)
$$
and remembering that you only care about the real part.
If you do so, you'll end up with terms such as $\cos \hat{x} - \sin \hat{y}$ which just so happen to again be the
a $\sin$ and a $\cos$ (our original plane wave components) shifted by $\pi/2$.

This polarization vector corresponds to a "\textbf{right handed}" field.

To be a "\textbf{left handed}" polarization vector, we would write
$$
\epsilon^\mu = (0, 1, -i, 0)
$$

Corresponding to
$$
\vect{E} = E_0 \left( \cos(wt-kz)\hat{x} + \sin(wt-kz)\hat{y} \right)
$$

J. Binney section 1.4.4 is also a great reference.


\subsection{Cross Sections}

The next thing we want to document is how to solve the differential cross section (per unit solid angle).
The expression given was

$$
\frac{d\sigma}{d\Omega} = \frac{\alpha^2}{4E_{cm}^{2}} (1 + \cos^2 \theta)
$$

That when integrated gives the total cross section
$$
\sigma_{total} = \frac{4\pi\alpha^2}{3E_{cm}^{2}}
$$

The trick here is to identity $d\Omega = \sin\theta \, d\theta \, d\phi$.
So essentially the problem is to integrate

\begin{align*}
\int d\Omega \, \left( 1 + \cos\theta \right) &=
    \int_{\phi=0}^{2\pi} \int_{\theta=0}^{\pi} \left( 1 + \cos\theta \right) \sin\theta \, d\theta \, d\phi \\
&= \left( \int_{\phi=0}^{2\pi} \int_{\theta=0}^{\pi} \sin\theta \, d\theta \, d\phi \right)
    + \left( \int_{\phi=0}^{2\pi} \int_{\theta=0}^{\pi} \cos\theta \sin\theta \, d\theta \, d\phi \right) \\
&= \left( 4\pi \right) + \left( -\int u^2 du \right) \\
&= \left( 4\pi \right) + \left( -\frac{1}{3} \cos^3 \theta \Big|_{0}^{\pi} du \right) \\
&= \left( 4\pi \right) \left( 2\pi \frac{8}{3} \right) \\
&= \frac{16\pi}{3}
\end{align*}

Note that we did a $u$-substitution in the second integral: $u = \cos\theta$, so $du = -\sin\theta \, d\theta$.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Klein-Gordon Field}

\subsection{Klein-Gordon Inconsistensies}

The Schrodinger equation canr readily be abtain treating the energy and momentum as operators.
In quantum mechanics $E = i\partial_t$ and $p = -i\nabla$.
Using the realtionship $E = \frac{p^2}{2}$, we get


$$
i\partial_t \ket{\psi} = -\frac{1}{2m} \nabla^2 \ket{\psi}
$$

Note that we are still in natural units, otherwise there would be an $\hbar$.

The Klein-Gordon equation comes if we instead use $E^2 - p^2 = m^2$,

$$
-\partial_{t}^2 \ket{\psi} + \nabla^2 \ket{\psi} = m^2 \ket{\psi}
$$

And remember that $\partial^2 = \partial_{t}^{2} - \nabla^2$, so there comes $(\partial^2 - m)\psi = 0$ equation.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Bit of Formalism: Fourier Transforms} \label{formalism:fourier-properties}

One method to solve the equation is to use Fourier transforms.
So we can begin by expressing the wave function $\phi(x,t)$ as a Fourier integral,

$$
\psi(x, t) =
\int \frac{d^4k}{\left(2\pi\right)^4} \tilde{\psi}(k, \omega) e^{-i(k\cdot x - \omega t)}
$$

This is actually the \textbf{inverse Fourier transform}.

Our convention of the \textbf{Fourier transform}, follows \cite{peskin-and-schroeder}:
$$
\mathcal{F} \{ \psi(x, t) \} =
\tilde{ \psi }(k, \omega) =
\int d^4x \, \psi(x, t) e^{i(k\cdot x - \omega t)}
$$


Here, $\tilde{\psi}(k, \omega)$ is the Fourier transform of $\psi(x, t)$.
One thing that's often missed is the argument of the exponent: if you look back to examples where we talk about plane waves
(go back to \ref{section:polarized-vectors}), a wave with positive momentum moving in positive direction $x$ has the argument $wt - xk$.
The most common convention is to have such a wave in the Fourier transform.

It is also worth stoping here to really take in and mess around with these expressions - we should understand
where that factor of $\left(2\pi\right)^n$ comes from becuase that will either simplify a ton
of expressions or it will get us an answer that's orders or magnitude off.

Let's work in four-dimensional space still but absorb the time time component in our notation
(eventhough we have already been doing this in our integration measures).

And consider this, if we take a function, then we compute its Fourier transform, and then we take its inverse
Fourier transform, then we should arrive at the original function.

\begin{align*}
\psi(x) &= \int \frac{d^4 k}{\left(2\pi\right)^4} \tilde{\psi}(k) e^{-ik\cdot x} \\
&= \int \frac{d^4 k}{\left(2\pi\right)^4}
    \left( \int d^4x^{\prime} \, \psi(x^\prime) e^{ik\cdot x^{\prime}} \right)
    e^{-ik\cdot x} \\
&= \int d^4x^{\prime} \, \psi(x^\prime)
    \left( \frac{d^4 k}{\left(2\pi\right)^4}  e^{ik\cdot \left(x^{\prime} - x\right)} \right)
\end{align*}

Thus, the only way this all works out is if
$$
\int \frac{d^4 k}{\left(2\pi\right)^4}  e^{ik\cdot \left(x^{\prime} - x\right)}
= \delta^{(4)} \left( x^{\prime} - x \right)
$$

If we try the same exercise but start with a function in momentum space, then do an inverse transform, followed by a Fourier
transform, then should again end with the same function.
You'll see why it is worth doing this exercise here in a bit.
Also note that we are following the convention of ignoring the tilde as we believe that now it will be clear
what is going on.

\begin{align*}
\psi(k) &= \int d^4 x \, \psi(x) e^{ik\cdot x} \\
&= \int d^4 x \, 
    \left( \int \frac{d^4 k^\prime}{\left(2\pi\right)^4} \psi(k^\prime) e^{-ik^\prime \cdot x} \right)
    e^{ik\cdot x} \\
&= \int \frac{d^4 k^\prime}{\left(2\pi\right)^4} \,\psi(k^\prime)
    \left( \int d^4 x \, e^{i(k - k^\prime)\cdot x} \right)
\end{align*}

Note that this time we left the $\left(2\pi\right)^4$ denominator with the momentum integral
in order to be consistent with out convention.
That is also why we organized the exponentials to have a leading "$+$" sign.

Note that in this case, for things to work out, we need

$$
\int d^4 x \, e^{i(k - k^\prime)\cdot x} = \left(2\pi\right)^4 \delta^{(4)} \left(k - k^\prime\right)
$$

Which is what \cite{peskin-and-schroeder} call out in the notations and conventions section.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Bit of Formalism: Normalization Factors in Momentum Integrals} \label{formalism:momentum-renormalization-factor}

We'll dive into a bit for formalism here before proceeding.

The reason we verified the Fourier transofrm above was because one often sees, in the
\textbf{position representation}, expressions such as

$$
\mathcal{I} = \int dx \, \ket{x}\bra{x}
$$

And one could naively plug into their equations the expression $\mathcal{I} = \int dp \, \ket{p}\bra{p}$.
But, the there is an important subtlety in our Fourier transform convention!
\\

If you recall from QM, $\ket{\psi}$ is meant to represent some generic state of a system
we are interested about.
This \textbf{state vector} $\ket{\psi}$ represents the possible quantum amplitudes for some particular
complete basis set in some particular coordinate system.
See \cite{binney}, chapters 1.2 and 1.4 on pages 9 and 15.

This way of defining the state vector $\ket{\psi}$
should ring a bell as it implicitly calls out to the \textbf{spectral theorem} for Hermitian operators.
Remember that the spectral theorem for spectral theorem states that the eigenvectors of a hermitian operator
form a complete, orthonormal set of basis sets.

Because any hermitian operator will provide for us a complete basis set, $\ket{\psi}$ as a generic
state of a system is a good abstraction.
And because of that, we use it to define the \textbf{wave function}, which gives us the amplitude to find
our system in a specific state.

For example, the wave function $\psi(x)$ is defined as:

$$
\psi(x) = \braket{x | \psi}
$$

This wave function, these amplitudes, can help us find the probability to find our system at position
$x \in \left( -\infty , \infty\right)$ and thus we can express our generic state in terms of the
complete basis set $\ket{x}$:

$$
\ket{\psi} = \int d^d x \, \psi(x) \ket{x} = \int d^d x \, \braket{x | \psi}  \ket{x}
$$

Another interesting wave function - another set of amplitudes - that we will be interested on
correspond to the amplitdues that a system with a definite momentum $p$ is found at $x$.
We can define this \textbf{position-representation of a momentum eigenstate} wave function as:

$$
u_p (x) = \braket{x | p}
$$
We are using this notation that comes from \cite{binney} because it helps us clearly see what a wave function
is as a set of amplitudes corresponding to the probability of a measurement but also
as a coordinate system.
Where in the position representation we denote $\psi(x)$ and $u_p (x)$ as functions of $x$
both defined by having a \textbf{dual vector}, $\bra{x}$ act on a vector.
Which is a great remainder that dual vectors are complex-valued (linear) functions whose domain is some
\textbf{vector space}.
\\

The reason this brief detour matters is because the wave function $u_p (x)$ appears a lot.
It appears so much that we actually begun talking about the Fourier transform which has us work out
our integrals in momentum space.

And the thing to know about these wave functions with definite momentum is that they carry a
normalization factor of $\left(2\pi\right)^{-d/2}$ per wave function, and there are always two
(because these are amplitdues).
So in QFT we usually absorve this $\left(2\pi\right)^{-d}$ factor into the Fourier transofmr instead of
carrying around with the wave function.
Specially since it isn't obvious, when using natural units, that this factor should be around.
Let's remember why it is so.

When you see the integrals in quantum mechanics,
the calculation often utilizes the well-known expressions for the overlap of position and momentum eigenstates
$\braket{\vect{x} | \vect{p}}$ (amplitude to find at $x$ a particle with well-defined momentum)
and $\braket{\vect{p} | \vect{x_0}}$ (amplitude to fina particle at $x_0$ with momentum $p$).
These expressions are crucial in converting between position and momentum representations,
which allows us to analyze the system's dynamics.

You may be familiar with the solution to the differential equation
$$
\braket{ x | \hat{p} | p} = -i \hbar \partial_x u_p (x) = p u_p(x)
$$
see \cite{binney} chapter 2.3.2, page 38.
The solutions are,

$$
\braket{\vect{x} | \vect{p}} = \frac{1}{(2\pi)^{3/2}} e^{i\vect{p}\cdot\vect{x}}
$$
and 
$$
\braket{\vect{p} | \vect{x_0}} = \frac{1}{(2\pi)^{3/2}} e^{-i\vect{p}\cdot\vect{x_0}}
$$


As we mention above, Pesking and schroeder use
$$
\braket{\vect{x} | \vect{p}} = e^{i\vect{p}\cdot\vect{x}}
$$
and 
$$
\braket{\vect{p} | \vect{x_0}} = e^{-i\vect{p}\cdot\vect{x_0}}
$$

It'd be rough to remember to find the missing normalization factor when using natural units,
people just add it to the momentum integrals, where they should always be.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A Bit of Formalism: Bras and the Dual Space} \label{formalism:bras}


First, this comes from the section "outer products" in
\href{https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation}{Wikipedia: Bra-Ket Notation},
we have to keep in mind that $\ket{\psi}\bra{\psi}$ defines an \textbf{outer product}.
In a finite-dimensional vector space the outer product is defined as

$$
\ket{\phi}\bra{\psi}
=
\begin{pmatrix}
    \phi_1 \\
    \phi_2 \\
    \vdots \\
    \phi_N 
\end{pmatrix}
\begin{pmatrix}
    \psi_{1}^{*} & \psi_{2}^{*} & \ldots & \psi_{N}^{*}
\end{pmatrix}
=
\begin{pmatrix} 
    \phi_1 \psi_{1}^{*} & \phi_1 \psi_{2}^{*} & \dots  & \phi_1 \psi_{N}^{*} \\
    \phi_2 \psi_{1}^{*} & \phi_2 \psi_{2}^{*} & \dots  & \phi_2 \psi_{N}^{*} \\
    \vdots              & \vdots              & \ddots & \vdots              \\
    \phi_N \psi_{1}^{*} & \phi_N \psi_{2}^{*} & \dots  & \phi_N \psi_{N}^{*} \\
\end{pmatrix}
$$

One of the uses of the outer product is to construct \textbf{projection operators}.

Given a ket $\ket{\psi}$ of norm 1 (being a complete basis set for some space),
the orthogonal projection onto the subspace spanned by $\ket{\psi}$ is
$\ket{\psi}\bra{\psi}$.

Very fancy way of saying that the outer product is a dual vector being used to define an operator.
Which may be weird given that thus far we've only mentioned that dual vectors are complex-valued linear
functions that feed on kets and spit out complex numbers, see \ref{formalism:momentum-renormalization-factor},
but in this presentation, the dual vector and the vector waiting for another vector to show up.
And when that vector does, the dual vector will appear, convert one of the vectors into a complex number
and scale the remaining ket.



The "Unit operator" section on \href{https://en.wikipedia.org/wiki/Bra%E2%80%93ket_notation}{Wikipedia: Bra-Ket Notation},
also has this: if we have a complete orthonormal basis $\{ e_i | i\in\mathbb{N} \}$,
functional analysis tells us that any $\ket{\psi}$ can also be written as
\begin{equation}
\ket{\psi} =
\sum_{i\in\mathbb{N}} \braket{e_i | \psi} \ket{e_i} \label{qm-formalism:discrete-basis}
\end{equation}

This is how we "project" $\psi$ into a new basis.

It mentions that it can also be shown that
$$
\mathbb{I} = \sum_{i\in\mathbb{N}} \ket{e_i} \bra{\psi}
$$

There is also this result called
\href{https://en.wikipedia.org/wiki/Borel_functional_calculus#Resolution_of_the_identity}{resolution of the identity in Borel functional calculus}
that allows us to generalized this result to the continuous case,

$$
\mathbb{I} = \int dx \, \ket{x} \bra{x} = \int dp \, \ket{p} \bra{p}
$$


The analogous to \ref{qm-formalism:discrete-basis} in the continuous case is
$$
\ket{\psi} = \int dx\, \psi(x) \ket{x}
$$
where $\psi (x) = \braket{x | \psi}$ by analogy.
\\



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Klein-Gordon Solution}

With this Fourier transofrm in place, we can take the Fourier transform of our entire
Klein-Gordin equation to reduce it to an algebraic problem.

Technically, what we will do is take the inverse Fourier transform of the first term in the KG equation,
$\partial_t \psi$.
This comes in handy and you will see why at the end.

The physical reason to go about it this way is because we want to reconstruct a function from its frequency
components (inverse transform).

\begin{align*}
\mathcal{F}^{-1}\{ \partial_t \psi \} &=
    \int \frac{d^4k}{(2\pi)^4} \partial_t \psi e^{-i(\vect{k}\cdot\vect{x} - \omega t)}
\rightarrow
\left[
    \begin{alignedat}{2}
        u  &= e^{-i(\vect{k}\cdot\vect{x} - \omega t)}                \quad & v  &= \psi \\
        du &= i\omega e^{-i(\vect{k}\cdot\vect{x} - \omega t)} \, dt  \quad & dv &= \partial_t \psi \, dt
    \end{alignedat}\,
\right] \\
&= \cancelto{0}{ \psi(\vect{x}, t) e^{-i(\vect{k}\cdot\vect{x} - \omega t)} } \Big|_{t=-\infty}^{t=\infty}
    - \int \frac{d^4k}{(2\pi)^4} \psi(\vect{x}, t) \left(i\omega\right) e^{-i(\vect{k}\cdot\vect{x} - \omega t)} \\
&= -i\omega \tilde{\psi} (\vect{k}, \omega)
\end{align*}

We made use of the integration by parts technique ($\int u \, dv = uv - \int v \, du$) to transfer the time derivative
on $\psi$ to the exponent term.

As per the boudnary term, we have a couple words to say: physically, the wave function (or even field) should vanish at infinity.
Mathematically, in order for the Fourier transform to converge to a value the function that is being "transformed", $\psi$,
must be a \textbf{rapidly decreasing function} (a function of \href{https://en.wikipedia.org/wiki/Schwartz_space}{Schwartz space}).

The forwared Fourier transform would be (decomposing the function into its frequency components):

\begin{align*}
\mathcal{F}\{ \partial_t \psi \} &=
    \int d^4x\, \partial_t \psi e^{i(\vect{k}\cdot\vect{x} - \omega t)}
\rightarrow
\left[
    \begin{alignedat}{2}
        u  &= e^{i(\vect{k}\cdot\vect{x} - \omega t)}                \quad & v  &= \psi \\
        du &= -i\omega e^{i(\vect{k}\cdot\vect{x} - \omega t)} \, dt  \quad & dv &= \partial_t \psi \, dt
    \end{alignedat}\,
\right] \\
&= \cancelto{0}{ \psi(\vect{x}, t) e^{i(\vect{k}\cdot\vect{x} - \omega t)} } \Big|_{t=-\infty}^{t=\infty}
    - \int d^4x\, \psi(\vect{x}, t) \left(-i\omega\right) e^{i(\vect{k}\cdot\vect{x} - \omega t)} \\
&= i\omega \tilde{\psi} (\vect{k}, \omega)
\end{align*}



Similarly then,

\begin{align*}
\mathcal{F}^{-1}\{ \partial_{t}^2 \psi \} &=
    \int \frac{d^4k}{(2\pi)^4} \partial_{t}^{2} \psi e^{-i(\vect{k}\cdot\vect{x} - \omega t)}
\rightarrow
\left[
    \begin{alignedat}{2}
        u  &= e^{-i(\vect{k}\cdot\vect{x} - \omega t)}                \quad & v  &= \partial_t \psi \\
        du &= i\omega e^{-i(\vect{k}\cdot\vect{x} - \omega t)} \, dt  \quad & dv &= \partial_{t}^{2} \psi \, dt
    \end{alignedat}\,
\right] \\
&= \cancelto{0}{ \partial _t \psi(\vect{x}, t) e^{-i(\vect{k}\cdot\vect{x} - \omega t)} } \Big|_{t=-\infty}^{t=\infty}
    - \int \frac{d^4k}{(2\pi)^4} \partial_t \psi(\vect{x}, t) \left(i\omega\right) e^{-i(\vect{k}\cdot\vect{x} - \omega t)} \\
&= -i\omega \mathcal{F}^{-1}\{ \partial_t \psi \} \\
&= - \omega^2 \tilde{\psi} (\vect{k}, \omega)
\end{align*}

The boundary term again goes to zero here.
We will skip the physical argument and just mention that a rapidly decreasin function also requires all of its derivatives
tend to zero.



For the spatial derivatives, each $\partial_i$ will bring down a factor of $-i k_i$, resulting in a factor of
$- k_{i}^2$ for each $\partial_{i}^{2}$.
And this is something that always triped me up: one may be inclined to write $-k^2$ in the Fourier transform
but that same one ought to remember that $k$ is actually $\vec{k}$ (a vector) and that is why we must
write $-|k|^2$, because we want the [L-2] norm.

But anyways, the Fourier transform of the Laplacian is,

\begin{align*}
\mathcal{F}^{-1}\{ \nabla \psi \} &=
    \int \frac{d^4k}{(2\pi)^4} \nabla \psi e^{-i(\vect{k}\cdot\vect{x} - \omega t)}
\rightarrow
\left[
    \begin{alignedat}{2}
        u  &= e^{-i(\vect{k}\cdot\vect{x} - \omega t)}                          \quad & v  &= \psi \\
        du &= -i\vect{k} e^{-i(\vect{k}\cdot\vect{x} - \omega t)} \, d\vect{x}  \quad & dv &= \nabla \psi \, d\vect{x}
    \end{alignedat}\,
\right] \\
&= \cancelto{0}{ \psi(\vect{x}, t) e^{-i(\vect{k}\cdot\vect{x} - \omega t)} } \Big|_{\vect{x}=-\infty}^{\vect{x}=\infty}
    - \int \frac{d^4k}{(2\pi)^4} \psi(\vect{x}, t) \left(-i\vect{k}\right) e^{-i(\vect{k}\cdot\vect{x} - \omega t)} \\
&= -i\vect{k} \tilde{\psi} (\vect{k}, \omega)
\end{align*}


and

\begin{align*}
\mathcal{F}^{-1}\{ \nabla^2 \psi \} &=
    \int \frac{d^4k}{(2\pi)^4} \nabla^2 \psi e^{-i(\vect{k}\cdot\vect{x} - \omega t)}
\rightarrow
\left[
    \begin{alignedat}{2}
        u  &= e^{-i(\vect{k}\cdot\vect{x} - \omega t)}                          \quad & v  &= \nabla \psi \\
        du &= -i\vect{k} e^{-i(\vect{k}\cdot\vect{x} - \omega t)} \, d\vect{x}  \quad & dv &= \nabla^2 \psi \, d\vect{x}
    \end{alignedat}\,
\right] \\
&= \cancelto{0}{ \nabla\psi(\vect{x}, t) e^{-i(\vect{k}\cdot\vect{x} - \omega t)} } \Big|_{\vect{x}=-\infty}^{\vect{x}=\infty}
    - \int \frac{d^4k}{(2\pi)^4} \nabla\psi(\vect{x}, t) \left(-i\vect{k}\right) e^{-i(\vect{k}\cdot\vect{x} - \omega t)} \\
&= -i\vect{k} \mathcal{F}^{-1}\{ \nabla \psi \} \\
&= - |\vect{k}|^2 \tilde{\psi} (\vect{k}, \omega)
\end{align*}



And since the above must hold throughout all of space, that's where
$$
\left(-\omega^2 + |\vect{k}|^2 + m^2 \right) \tilde{\psi}(\vect{k}, \omega) = 0
$$
comes from!

Now, in order for us to not have a trivial solution, $\psi(\vect{x}, t) = 0$, it must be so that
$\left(-\omega^2 + |\vect{k}|^2 + m^2 \right) = 0$, and so our dispersion relation comes about
$$
\omega^2 = |\vect{k}|^2 + m^2
$$
So $\omega = \pm \sqrt{ |\vect{k}|^2 + m^2 }$.
So essentially any function will do as long as the dispersion relation (momentum conservation) is respected.

A thing that people do, specially since we are talking about Fourier transforms is to define

$$
\psi(\vect{k}, \omega) = A(\vec{k}) \delta\left( \omega - \sqrt{ |\vect{k}|^2 + m^2 }\right) +
    B(\vec{k}) \delta\left( \omega + \sqrt{ |\vect{k}|^2 + m^2 }\right)
$$

Note that $A(\vec{k})$ and $B(\vec{k})$ are arbitrary functions that only depend on $\vect{k}$ since the
Dirac delta functions specifies the value for $\omega$.

With that, we can finally write a solution for the differential equation we started with,

\begin{align*}
\psi(\vect{x}, t) &=
    \int \frac{d^4k}{(2\pi)^4} \left[
        A(\vec{k}) \delta\left( \omega - \sqrt{ |\vect{k}|^2 + m^2 }\right) e^{-i(\vect{k}\cdot\vect{x} -\omega t)} +
        B(\vec{k}) \delta\left( \omega + \sqrt{ |\vect{k}|^2 + m^2 }\right) e^{-i(\vect{k}\cdot\vect{x} -\omega t)}
    \right] \\
&= \int \frac{d^4k}{(2\pi)^4} \left[
    A(\vec{k}) e^{-i(\vect{k}\cdot\vect{x} - \sqrt{ |\vect{k}|^2 + m^2 } t)} +
    B(\vec{k}) e^{-i(\vect{k}\cdot\vect{x} + \sqrt{ |\vect{k}|^2 + m^2 } t)}
\right]
\end{align*}

Hopefully thiis result makes sense: we obtained a family of function as solution and only initial conditions or
boundary conditions will result in a specific sort of function.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Klein-Gordon Negative Density}


The expression to compute a probability current is derived by using the analogous continuity equation
from fluid dynamics

$$
\frac{\partial\rho}{\partial t} + \nabla\cdot \vect{j} = 0
$$


Using the Schrodinger equation and using the fact that $\rho = |\psi|^2 = \psi^* \psi$ one
can arrive at the probability density
$$
\rho = 
-\frac{i}{2m} \left( \psi^* \partial_t \psi - \psi \partial_t \psi^* \right)
$$

For the Klein Gordon equation, the way to massage it and get an expression for $\partial_t \rho$
and for its probability current is to:
take Klein-Gordon multiply it by $\psi^*$,
take the complex conjugate multiply it by $\psi$,
subtract the two and rearange to get something like the continuity equation.

Following those steps we have,

$$
\left( \partial_{t}^{2} - \nabla^2 + m^2 \right) \psi = 0
$$
and
$$
\left( \partial_{t}^{2} - \nabla^2 + m^2 \right) \psi^* = 0
$$

Multiplying them with $\psi^*$ and $\psi$ respectively we get,
$$
\psi^* \partial_{t}^{2} \psi - \psi^* \nabla^2 \psi + \psi^* m^2 \psi = 0
$$
and
$$
\psi \partial_{t}^{2} \psi^* - \psi \nabla^2 \psi^* + \psi m^2 \psi^* = 0
$$

Subtracting the latter from the former,
\begin{align*}
& \psi^* \partial_{t}^{2} \psi - \psi^* \nabla^2 \psi + \psi^* m^2 \psi -
    \psi \partial_{t}^{2} \psi^* + \psi \nabla^2 \psi^* - \psi m^2 \psi^* \\
&= \left( \psi^* \partial_{t}^{2} \psi - \psi \partial_{t}^{2} \psi^* \right)
    - \left( \psi^* \nabla^2 \psi - \psi \nabla^2 \psi^* \right)
    + \left( \psi^* m^2 \psi - \psi m^2 \psi^* \right) \\
&= \left( \psi^* \partial_{t}^{2} \psi - \psi \partial_{t}^{2} \psi^* \right)
    - \left( \psi^* \nabla^2 \psi - \psi \nabla^2 \psi^* \right) \\
&= \partial_t \left( \psi^* \partial_{t} \psi - \psi \partial_{t} \psi^* \right)
    -\nabla \cdot \left( \psi^* \nabla \psi - \psi \nabla \psi^* \right)
\end{align*}

The time derivative is then equated to $\partial_t \rho$ and the spatial derivatives to the current.
though one interesting tangent to take here is concerning the missing $\frac{i}{2m}$ factor that
these equations have.

If you remember, the imaginery part of of a complex number can be isolated by taking the different of it with its
complex conjugate: $\im{z} = \frac{1}{2i}(z - \overline{z})$. (Recall that if $z = a +ib$, then $\im{z} = b$, not $ib$.)
It just so happens that we are doing the same sort of operation here, so we can throw an $i/2$ factor into our equation
to ensure we get a real quantity.

For the $1/m$ factor we have to do some dimensional analysis.
To get the probability we have to integrate $\int d^3x \, |\psi|^2$, which is dimensionless.
So the $|\psi|^2$ term needs to cancel out the integration over space.
Hence $[|\psi|^2] = [L]^{-3} = [M]^{3}$.
From there we can say that $[\psi] = [L]^{-3/2}$ and
$[\psi^* \partial_{t}^2 \psi] = [L]^{-3/2}[L]^{-2}[L]^{-3/2} = [L]^{-5} = [M]^{5}$.

This same term we just evaluated needs to match the dimensions of
the time derivative, $[L]^{-1}$,a probability density, $[L]^{-3}$.
It should be then that $[\partial_t \rho] = [L]^{-4} = [M^4]$.
But hey, what a coincidence that if we were to divide $\psi^* \partial_{t}^2 \psi$ by a mass
that we would get just the right dimensions for our probability current!
And so it goes that the expression we are used to seeing turns out to be excused as such in the Klein-Gordon case.
\\


Suppose we had a $\psi =  e^{-i(\vect{k}\cdot\vect{x} -\omega t)}$ as solution.

Then,
\begin{align*}
\rho &=
    -\frac{i}{2m} \left( 
        e^{i(\vect{k}\cdot\vect{x} -\omega t)} \partial_t e^{-i(\vect{k}\cdot\vect{x} -\omega t)} -
        e^{-i(\vect{k}\cdot\vect{x} -\omega t)} \partial_t e^{i(\vect{k}\cdot\vect{x} -\omega t)} 
    \right) \\
&= -\frac{i}{2m} \left(
    e^{i(\vect{k}\cdot\vect{x} -\omega t)} \left(i\omega\right) e^{-i(\vect{k}\cdot\vect{x} -\omega t)} -
    e^{-i(\vect{k}\cdot\vect{x} -\omega t)} \left(-i\omega\right) e^{i(\vect{k}\cdot\vect{x} -\omega t)}
    \right) \\
&= -\frac{i}{2m} \left(i\omega\right) \left( 2 \right) \\
&= \frac{\omega}{m}
\end{align*}

And since $\omega = \pm \sqrt{|\vect{k}|^ + m^2}$, then the density can be negative!





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Causality Arguments: The Non-relativistic Case}

Consider the amplitude for a free particle to propagate from $\vect{x}_0$ to $\vect{x}$
$$
U(t) = \bra{\vect{x}} e^{-iHt} \ket{\vect{x}_0}
$$

In nonrelativistic quantum mechanics we have $E = \frac{1}{2m} p^2$, so
\begin{align}
U(t) &= \bra{\vect{x}} e^{-i(\vect{p}^2/2m)t} \ket{\vect{x}_0} \label{kg:nonrelativistic-01} \\
&= \int \frac{d^3p}{\left(2\pi\right)^3} \, \bra{\vect{x}} e^{-i(\vect{p}^2/2m)t} \ket{\vect{p}} \braket{\vect{p} | \vect{x}_0} \label{kg:nonrelativistic-02} \\
&= \int \frac{d^3p}{(2\pi)^3} e^{-i(\vect{p}^2/2m)t} e^{i\vect{p}\cdot(\vect{x}-\vect{x}_0)} \label{kg:nonrelativistic-03} \\
&= \left( \frac{m}{2\pi i t} \right)^{3/2} e^{im (\vect{x}-\vect{x}_0)^2 /2t} \label{kg:nonrelativistic-04}
\end{align}


To go from \ref{kg:nonrelativistic-01} to \ref{kg:nonrelativistic-02},
we changed the basis to momentum eigenstates in order to apply the Hamiltonian operator and "extract" the exponential.
If we have the operator $\hat{p}$ act on a pure momentum eigenstate $\ket{p}$, then we get $\hat{p}\ket{p} = p\ket{p}$.
So changing basis helps us work with real quantities
(an eigenvalue obtained by applying a Hermitian operator on its eigenket).

Note that one may want to use the identity $\mathbb{I} = \int dp\, \ket{p}\bra{p}$.
But as we saw in \ref{formalism:fourier-properties} and in \ref{formalism:momentum-renormalization-factor}, in order to stay consistent, we need our integrals over
momentum space to have denominators of $\left(2\pi\right)^d$.
So the actual identity we should use, which is what Peskin and Schroeder use, is identity
$\mathbb{I} = \int \frac{d^dp}{(2\pi)^d} \ket{p}\bra{p}$.
The additional $1/(2\pi)^d$ factor picks up the normalization for factors such as $\braket{p | x_0}$.



When you see the integrals over states in quantum mechanics, especially when involving time evolution and position states,
the calculation often utilizes the well-known expressions for the overlap of position and momentum eigenstates
$\braket{\vect{x} | \vect{p}}$ and $\braket{\vect{p} | \vect{x_0}}$.
These expressions are crucial in converting between position and momentum representations,
which allows us to analyze the system's dynamics.
In such cases, we have these useful identities:

$$
\braket{\vect{x} | \vect{p}} = \frac{1}{(2\pi)^{3/2}} e^{i\vect{p}\cdot\vect{x}}
$$
and 
$$
\braket{\vect{p} | \vect{x_0}} = \frac{1}{(2\pi)^{3/2}} e^{-i\vect{p}\cdot\vect{x_0}}
$$

But in QFT we instead use:
$$
\braket{\vect{x} | \vect{p}} = e^{i\vect{p}\cdot\vect{x}}
$$
and 
$$
\braket{\vect{p} | \vect{x_0}} = e^{-i\vect{p}\cdot\vect{x_0}}
$$
This way, it is not the projection into momentum space that carries the appropiate normalization factor
but the Fourier integrals.
Which is a bit more natural when working with natural units.


We used these identities to go from \ref{kg:nonrelativistic-02} to \ref{kg:nonrelativistic-03},

\begin{align*}
& \int \frac{d^3p}{(2\pi)^3} e^{-i(\vect{p}^2/2m)t} \braket{\vect{x} | \vect{p}} \braket{\vect{p} | \vect{x}_0} \\
&= \int \frac{d^3p}{(2\pi)^3} e^{-i(\vect{p}^2/2m)t} \frac{1}{(2\pi)^3} e^{i\vect{p}\cdot(\vect{x}-\vect{x}_0)} 
\end{align*}


As per the solution, see \ref{ps-kg:nonrelativistic-propagator}.
The trick is to complete the square for the arguments of the exponentials and extract a Gaussian integral.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Causality Arguments: The Relativistic Case}

And now let's look at
\begin{align*}
U(t) &= \bra{\vect{x}} e^{-it\sqrt{\vect{p}^2 + m^2}} \ket{\vect{x}_0} \\
&= \int \frac{d^3p}{\left(2\pi\right)^3} \, \bra{\vect{x}} e^{-it\sqrt{\vect{p}^2 + m^2}} \ket{\vect{p}} \braket{\vect{p} | \vect{x}_0} \\
&= \int \frac{d^3p}{\left(2\pi\right)^3} \, e^{-it\sqrt{\vect{p}^2 + m^2}} \braket{\vect{x} | \vect{p}} \braket{\vect{p} | \vect{x}_0} \\
&= \int \frac{d^3p}{\left(2\pi\right)^3} \, e^{-it\sqrt{\vect{p}^2 + m^2}} e^{i\vect{p}\cdot\vect{x}} e^{-i\vect{p}\cdot\vect{x}_0} \\
&= \int \frac{d^3p}{\left(2\pi\right)^3} \, e^{-it\sqrt{\vect{p}^2 + m^2}} e^{i\vect{p}\cdot(\vect{x} - \vect{x}_0)} \\
\end{align*}

The result then happens to be
$$
\frac{1}{2\pi^2 |\vect{x}-\vect{x}_0|}
    \int_{0}^{\infty} dp \, p \sin(p|\vect{x}-\vect{x}_0|) e^{-it\sqrt{\vect{p}^2 + m^2}}
$$

The trick for this one is to use spherical variables to simplify it a bit.
$$
\int \frac{d^3p}{(2\pi)^3} \, e^{-it\sqrt{\vect{p}^2 + m^2}} e^{i\vect{p}\cdot(\vect{x} - \vect{x}_0)} 
$$
becomes
$$
\int \frac{dp}{(2\pi)^3} d\Omega \, p^2 e^{-it\sqrt{p^2 + m^2}} e^{ip\vect{r}\cos\theta} 
$$
where $r = |\vect{x} - \vect{x}_0|$ and $p = |\vect{p}|$.

From here we can do the angular integrals first,

\begin{align*}
\int d\Omega \, e^{ipr\cos\theta} &= 
    \int_{0}^{2\pi} d\phi \int_{0}^{\pi} d\theta \, \sin\theta e^{ipr\cos\theta} \\
&= 2\pi \int_{0}^{\pi} d\theta \, \sin\theta e^{ipr\cos\theta} \\
&= -2\pi \int_{1}^{-1} du \, e^{ipr u} \\
&= -2\pi \left( \frac{ e^{ipr u} }{ ipr } \right) \Big|_{u=1}^{u=-1} \\
&= -\frac{2\pi}{ipr} \left( e^{-ipr} - e^{ipr} \right)
\end{align*}

Next, we can make use of the formula $\im{z} = \frac{z - \overline{z}}{2i}$ and apply it to $z = e^{ix}$.
Or just do it manually: $e^{ix} = \cos + i\sin$, $e^{-ix} = \cos -i\sin$, so
$e^{-ix} - e^{ix} = -i\sin -i\sin = -2i \sin$.

\begin{align*}
\int d\Omega \, e^{ipr\cos\theta} &= 
    -\frac{2\pi}{ipr} \left( e^{-ipr} - e^{ipr} \right) \\
&= -\frac{2\pi}{ipr} \left(-2i \sin(pr) \right) \\
&= \frac{4\pi}{pr} \sin(pr)
\end{align*}

Pluging this back in,
We got
\begin{align}
\int \frac{dp}{(2\pi)^3} d\Omega \, |p|^2 e^{-it\sqrt{p^2 + m^2}} e^{ipr\cos\theta} &=
\int \frac{dp}{(2\pi)^3} p \frac{4\pi}{r} \sin(pr) e^{-it\sqrt{p^2 + m^2}} \\
&= \frac{1}{2\pi^2 |\vect{x}-\vect{x}_0|}
    \int_{0}^{\infty} dp \, p \sin(p|\vect{x}-\vect{x}_0|) e^{-it\sqrt{p^2 + m^2}} \label{kg-relativistic-simplified}
\end{align}

The reference to Gradsheteyn and Ryzhik to 3.914 lists the following: on page 491, item 6 has
\begin{equation}
\int_{0} dx\, x \sin(bx) e^{-\beta \sqrt{x^2 + \gamma^2}} =
    \frac{b \beta \gamma^2}{\beta^2 + b^2} K_2\left(\gamma \sqrt{b^2 + \beta^2}\right) \label{Gradsheteyn-kg-relativistic}
\end{equation}

ET I 175(35).
I refers to volume 1 of the reference, 175 is the page where it should be found, (35) refers to the number of the formula
in that work.

We found a PDF of ET and on said place we found ourselves in the Laplace transforms section.
This section has two columns, one of them is $f(t)$ and the other one is $g(p) = \int_{0}^{\infty} dt\, f(t) d^{-pt}$.
Forumal 35 had on the first column
$$
t^\alpha L_{n}^{\alpha} (\lambda t) L_{m}^{\alpha} (k t)
$$
for $\re{\alpha} > -1$.

On the second column was the following:
\begin{align*}
\frac{\Gamma(m+n+\alpha + 1)}{m! n!}
\frac{(p-\lambda)^n (p-k)^m}{p^{m+n+\alpha +1}}
{}_{2}F_{1} \left[ -m, -n; -m-n-\alpha; \frac{p(p-\lambda -k)}{(p-\lambda)(p-k)} \right]
\end{align*}
When $\re{p} > 0$.
\\

Anyway, backing up a bit, $K_\nu (z)$ is a Bessel function for an imaginary argument.
Which is defined in 8.407 and in 8.43, on page 911.
Gradsheteyn and Ryzhik have the following definitions.
If $-\pi < \arg{z} \leq \frac{1}{2}\pi$, then
$$
K_\nu (z) = \frac{i\pi}{2} e^{i\pi\nu /2} H_{\nu}^{(1)} \left(z e^{\frac{1}{2} i\pi} \right)
$$

If $-\frac{1}{2}\pi < \arg{z} \leq \pi$, then
$$
K_\nu (z) = -\frac{i\pi}{2} e^{-i\pi\nu /2} H_{-\nu}^{(2)} \left(z e^{-\frac{1}{2} i\pi} \right)
$$

$H_{\nu}^{(1)} (z)$ and $H_{\nu}^{(2)} (z)$ are Bessel function of the third kind, or Hankel functions.
Which are defined in terms of the Bessel functions of the first kind, $J_{\nu}(z)$, and on Bessel functions
of the second kind, $Y_\nu (z)$ (also called Neumman functions and written as $N_\nu (z)$).

$$
H_{\nu}^{(1)} (z) = J_\nu (z) + i Y_\nu (z)
$$
$$
H_{\nu}^{(2)} (z) = J_\nu (z) - i Y_\nu (z)
$$

Comparing \ref{kg-relativistic-simplified} with \ref{Gradsheteyn-kg-relativistic},
\begin{itemize}
    \item $x \rightarrow p$
    \item $b \rightarrow |x-x_0|$
    \item $\beta \rightarrow it$
    \item $\gamma \rightarrow m$
\end{itemize}

So
$$
\frac{b \beta \gamma^2}{\beta^2 + b^2} K_2\left(\gamma \sqrt{b^2 + \beta^2}\right)
$$
becomes
$$
\frac{it m^2 |x-x_0| }{-t^2 + |x-x_0|^2} K_2\left(m \sqrt{ -t^2 + |x-x_0|^2}\right)
$$

Now let's look at the modified function $K_2$,
\begin{align*}
K_2\left(m \sqrt{ -t^2 + |x-x_0|^2}\right) &=
    \frac{i\pi}{2} e^{i\pi} H_{2}^{(1)} \left(m \sqrt{ -t^2 + |x-x_0|^2} e^{\frac{1}{2} i\pi} \right) \\
    &= - \frac{i\pi}{2} H_{2}^{(1)} \left(m \sqrt{ -t^2 + |x-x_0|^2} \right)
\end{align*}

Looking at Gradsheteyn and Ryzhik again,
$$
H_{2}^{(1)} \left(m \sqrt{ -t^2 + |x-x_0|^2} \right) =
    J_2 \left(m \sqrt{ -t^2 + |x-x_0|^2} \right) + i Y_2 \left(m \sqrt{ -t^2 + |x-x_0|^2} \right)
$$

And here we stop and appreciate why the method of stationary phase was mentioned and use by the authors.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{On The Way To The Method of Stationary Phase: Laplace's Method}


$$
\mathcal{I}(\lambda) = \int_{a}^{b} dx \, f(x) e^{i\lambda S(x)}
$$

When $\lambda$ is large, the phase factor, the exponential, oscillates very rapidly,
except near the stationary points where $S^{\prime} (x) = 0$.

Let's call a stationary point $x_0$.
A Taylor expansion around it would be
$$
S(x) =
    S(x_0) + S^{\prime} (x_0) (x - x_0) + \frac{1}{2!} S^{\prime \prime} (x_0) (x - x_0)^2 + \dots
$$

The linear term vanishes since we are in a stationary point.
Substituting this approximation into our integral, we get

\begin{align*}
\mathcal{I}(\lambda) &\approx
    \int_{a}^{b} dx \, f(x) \exp \left( i\lambda \left(
        S(x_0) + \frac{1}{2} S^{\prime \prime} (x_0) (x - x_0)^2
    \right) \right) \\
&= \int_{a}^{b} dx \, f(x)
    e^{ i\lambda S(x_0) }
    \exp \left( \frac{i \lambda}{2} S^{\prime \prime} (x_0) (x - x_0)^2 \right) \\
&= e^{ i\lambda S(x_0) } \int_{a}^{b} dx \, f(x)
    \exp \left( \frac{i \lambda}{2} S^{\prime \prime} (x_0) (x - x_0)^2 \right)
\end{align*}


Another trick that is done is to expand the limits of integration to $\pm \infty$.
This can be justified because away from the stationary point, the phase factor
oscillations will cancel each other out.

So we now work on
$$
\mathcal{I}(\lambda) \approx
e^{ i\lambda S(x_0) } \int_{-\infty}^{\infty} dx \, f(x_0)
    \exp \left( \frac{i \lambda}{2} S^{\prime \prime} (x_0) (x - x_0)^2 \right)
$$

Note that since we are only interested in the neighborhood of the stationary point we are also
approximating $f(x) \approx f(x_0)$.

This integral is a complex Gaussian, see \ref{integrals:complex-gaussian}.
The general solution is

$$
\int e^{-a(t+b)^2} dt = \sqrt{\frac{\pi}{a}}
$$

Here $a = -i \frac{\lambda}{2} S^{\prime \prime} (x_0) $.
So

\begin{align*} \label{equation:stat-phase-solution}
\mathcal{I}(\lambda) \approx
    e^{ i\lambda S(x_0) } f(x_0)
    \sqrt{ \frac{2\pi}{ -i\lambda S^{\prime \prime} (x_0) } }
\end{align*}

This tells us that that for large $\lambda$,
the integral is dominated by contributions near the stationary points,
with amplitude decaying as $\lambda^{-1/2}$.

As for why the phase factor oscillates less rapidly near stationary points,
we need to look at the phase factor $ e^{ i\lambda S(x_0) } $ (the oscillating part!).

The rate of oscillation is determined by how quickly the phase $\lambda S(x)$ changes with $x$.
This rate of change is

$$
\frac{d}{dx} \left(\lambda S(x)\right) = \lambda S^{\prime} (x)
$$

Since $S^{\prime} (x_0) = 0$, then the phase factor is not changing.

This is why the main contribution to the integral comes from near the stationary points - everywhere else,
the rapid oscillations tend to cancel each other out when integrated.



Looking back at \ref{kg-relativistic-simplified},

\begin{align}
U(t) &= \bra{\vect{x}} e^{-iHt} \ket{\vect{x}_0} \\
&= \int \frac{dp}{(2\pi)^3} d\Omega \, |p|^2 e^{-it\sqrt{p^2 + m^2}} e^{ipr\cos\theta} \\
&= \frac{1}{2\pi^2 |\vect{x}-\vect{x}_0|}
    \int_{0}^{\infty} dp \, p \sin(p|\vect{x}-\vect{x}_0|) e^{-it\sqrt{p^2 + m^2}} 
\end{align}


Let's defined $x = \left| \vect{x}-\vect{x}_0 \right|$.
And remeber that $\sin\left(px\right) = \frac{1}{2i} \left(e^{ipx} - e^{-ipx}\right)$.
So we have,

\begin{align}
U(t) &= \bra{\vect{x}} e^{-iHt} \ket{\vect{x}_0} \\
&= \frac{1}{2\pi^2 |\vect{x}-\vect{x}_0|}
    \int_{0}^{\infty} dp \, p \sin(p|\vect{x}-\vect{x}_0|) e^{-it\sqrt{p^2 + m^2}}  \\
&= \frac{1}{4i \pi^2 x}
    \int_{0}^{\infty} dp \, p \left(e^{ipx} - e^{-ipx}\right) e^{-it\sqrt{p^2 + m^2}} \\
&= \frac{1}{4i \pi^2 x}
    \int_{0}^{\infty} dp \, p
    \left[ e^{i \left(px - t\sqrt{p^2 + m^2}\right)} - e^{-i \left(px + t\sqrt{p^2 + m^2}\right)} \right]
\end{align}

Here, we need some physics, and \cite{peskin-and-schroeder}, chapter 2, page 14, only looks at the asymptotic behaviour
for $x^2 \gg t^2$.
In that region, the argument of the second exponent approaches zero.
So that term becomes a constant.

\begin{align}
U(t) &= \bra{\vect{x}} e^{-iHt} \ket{\vect{x}_0} \\
&= \frac{1}{4i \pi^2 x}
    \int_{0}^{\infty} dp \, p
    \left[ e^{i \left(px - t\sqrt{p^2 + m^2}\right)} - e^{-i \left(px + t\sqrt{p^2 + m^2}\right)} \right] \\
&\approx \frac{1}{4i \pi^2 x}
    \int_{0}^{\infty} dp \, p \,
    e^{i \left(px - t\sqrt{p^2 + m^2}\right)}
\end{align}

This last integral is perfect for the stationary phase method.
Let's find the stationary points of the phase factor now,

\begin{align*}
\frac{d}{dp} \left( px - t\sqrt{p^2 + m^2} \right) &= x - t\frac{p}{\sqrt{p^2 + m^2}} = 0
\end{align*}

To isolate $p_0$, we can try the following

$$
x = t\frac{p}{\sqrt{p^2 + m^2}}
$$

$$
x \sqrt{p^2 + m^2} = pt
$$

$$
x^2 \left( p^2 + m^2 \right) = p^2 t^2
$$

$$
x^2 p^2 + x^2 m^2 = p^2 t^2
$$

Which can finally be rewritten as

$$
p^2 \left(x^2 - t^2 \right) = -m^2 x^2
$$

or

$$
p_0 = \pm i \frac{mx}{ \sqrt{ x^2 - t^2 } }
$$


Substituting this back into the phase factor, we get

\begin{align*}
\exp\left( i \left(p_0 x - t\sqrt{p_{0}^{2} + m^2}\right) \right) &=
    \exp\left( i \left(\pm i \frac{mx^2}{ \sqrt{ x^2 - t^2 } } 
        - t\sqrt{-\frac{m^2 x^2}{x^2 - t^2} + m^2}\right) \right) \\
\end{align*}

Let's drop the exponent from the expression and continue working.

\begin{align*}
\mp \frac{mx^2}{ \sqrt{ x^2 - t^2 } }
    - i t \sqrt{m^2} \sqrt{-\frac{x^2}{x^2 - t^2} + 1}
&= \mp \frac{mx^2}{ \sqrt{ x^2 - t^2 } }
    - i t \sqrt{m^2 t^2} \sqrt{-1} \sqrt{ \frac{1}{x^2 - t^2} } \\
&= \mp m x^2 \frac{1}{ \sqrt{ x^2 - t^2 } }
    \pm m t^2 \frac{1}{ \sqrt{ x^2 - t^2 } } \\
&= \mp m \sqrt{x^2 - t^2}
\end{align*}


Since \cite{peskin-and-schroeder} chose to push the contour upward so that it goes through
$p_0 = + i \frac{mx}{ \sqrt{ x^2 - t^2 } } $, then we get the exponent term that looks like
$$
e^{- m \sqrt{x^2 - t^2}}
$$


Recall from \ref{equation:stat-phase-solution}, that one of the terms in the answer is
$e^{ i\lambda S(x_0) }$, the exponential term at the phase factor, what we evaluated above,
hence Peskin and schroeder's answer.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Stationary Phase Method: Background}

To understand this method well it helps to first read
\href{https://en.wikipedia.org/wiki/Laplace%27s_method}{Wikipedia: Laplace's Method}.

If anything we think these are the highlights from Laplace's method.

If $f(x)$ has a unique global maximum at $x_0$ and $M>0$ and if we deifne the following functions
$$
g(x) = Mf(x)
$$
and
$$
h(x) = e^{g(x)} = e^{Mf(x)}
$$

Then look at the ratios
$$
\frac{g(x_0)}{g(x)} = \frac{f(x_0)}{f(x)}
$$
and
$$
\frac{h(x_0)}{h(x)} = e^{M(f(x_0) - f(x))}
$$

As $M$ increases, the ratio for $h$ will grow exponentially, while the ratio for $g$ does not change.
Since $f(x) \leq f(x_0)$, then the ratio of $h$ will always look like $e^{+{x}M}$.
Said another way, in places where the value of $f(x)$ is a lot smaller than $f(x_0)$, the ratio of $h$ will look like
$e^{Mf(x_0)}$, in places where the value of $f(x)$ is close to $f(x_0)$, the ratio will be smaller.
\\

But in general the trick is to expand $f(x)$ around $x_0$ following Taylor's theorem
$$
f(x) \approx
    f(x_0) + f^{\prime} (x_0) (x-x_0) + \frac{2}{2} f^{\prime\prime} (x_0) (x-x_0)^2 + R
$$

Since $x_0$ is a global maxima, then $f^{\prime} (x_0) = 0$ and $f^{\prime\prime} (x_0) < 0$.
Hence,
$$
f(x) \approx
    f(x_0) - \frac{1}{2} |f^{\prime\prime} (x_0)| (x-x_0)^2 + R
$$

So we have this approximation,
$$
\int_{a}^{b} e^{Mf(x)} dx \approx
    e^{Mf(x_0)} \int_{a}^{b} e^{- \frac{1}{2} M |f^{\prime\prime} (x_0)| (x-x_0)^2} dx
$$

So we are approximating the function with a Gaussian.
Here the last part of the approximation is to extend the limits of itnegration so that we do indeed have a Gaussian integral,
which we assume we can ebcause the tails decay quickly.
Since
$$
\int dx\, e^{-a(x+b)^2} = \sqrt{ \frac{\pi}{a} }
$$

$$
\int_{a}^{b} e^{Mf(x)} dx \approx
    e^{Mf(x_0)} \sqrt{ \frac{2\pi}{M |f^{\prime\prime} (x_0)|} }
$$

The next stepping stone is
\href{https://en.wikipedia.org/wiki/Method_of_steepest_descent}{Wikipedia: the method of steepest descent}.
This method build upon Laplace's method and aplies to contour integrals in the complex plane.
So now we are looking at approximating integrals such as
$$
\int_{C} f(z) e^{\lambda g(z)} dz
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{A Bit of Formalism: Understanding Tensors}

$$
\left( x \times y \right)^i = \sum_{j,k=1}^{n=3} \epsilon_{ijk} x_j y_k = \epsilon_{ijk} x_j y_k
$$

Expanding the sum it looks like
\begin{align}
    \begin{split}
\left( x \times y \right)^i =& \, \epsilon_{ijk} x_j y_k \\
=& \, \epsilon_{ij1} x_j y_1 + \epsilon_{ij2} x_j y_2 + \epsilon_{ij3} x_j y_3 \\
=& \, \epsilon_{i11} x_1 y_1 + \epsilon_{i12} x_1 y_2 + \epsilon_{i13} x_1 y_3 \\
    & \epsilon_{i21} x_2 y_1 + \epsilon_{i22} x_2 y_2 + \epsilon_{i23} x_2 y_3 \\
    & \epsilon_{i31} x_3 y_1 + \epsilon_{i32} x_3 y_2 + \epsilon_{i33} x_3 y_3 \\
=& \, 0 + \epsilon_{i12} x_1 y_2 + \epsilon_{i13} x_1 y_3 \\
    & \epsilon_{i21} x_2 y_1 + 0 + \epsilon_{i23} x_2 y_3 \\
    & \epsilon_{i31} x_3 y_1 + \epsilon_{i32} x_3 y_2 + 0 \\
    \end{split}
\end{align} \label{tensors:levi-civita-component-i}

The thing to notice here is that the above operation was just to obtain the $i$-th component of the cross product
of $x \times y$.

This also leads us to the notation where a tensor of rank 3 is to be fed 3 vectors,

$$
\epsilon (e_i, e_j, e_k) = \epsilon_{ijk} = e_i \cdot (e_j \times e_k)
$$

Where, again, we see that the rank 3 tensor takes 3 vectors as input and produces a number.
(a linear map or a functional one might say.)

From the explicit computation of the cross product we can also see that we obtain this matrix-like
structure from computing each of the $i$-th components
$$
\begin{pmatrix}
\epsilon_{i11} & \epsilon_{i12} & \epsilon_{i13} \\
\epsilon_{i21} & \epsilon_{i22} & \epsilon_{i23} \\
\epsilon_{i31} & \epsilon_{i32} & \epsilon_{i33}
\end{pmatrix}
$$

when $i=1$,
$$
\begin{pmatrix}
\epsilon_{111} & \epsilon_{112} & \epsilon_{113} \\
\epsilon_{121} & \epsilon_{122} & \epsilon_{123} \\
\epsilon_{131} & \epsilon_{132} & \epsilon_{133}
\end{pmatrix}
=
\begin{pmatrix}
0 & 0 & 0 \\
0 & 0 & 1 \\
0 & -1 & 0
\end{pmatrix}
$$

and for $n=3$ there are two other such matrices.
And such is the Levi-Civita tensor of rank 3!

The other two "layers" of the tensor are

$$
\begin{pmatrix}
\epsilon_{211} & \epsilon_{212} & \epsilon_{213} \\
\epsilon_{221} & \epsilon_{222} & \epsilon_{223} \\
\epsilon_{231} & \epsilon_{232} & \epsilon_{233}
\end{pmatrix}
=
\begin{pmatrix}
0 & 0 & -1 \\
0 & 0 & 0 \\
1 & 0 & 0
\end{pmatrix}
$$

$$
\begin{pmatrix}
\epsilon_{311} & \epsilon_{312} & \epsilon_{313} \\
\epsilon_{321} & \epsilon_{322} & \epsilon_{323} \\
\epsilon_{331} & \epsilon_{332} & \epsilon_{333}
\end{pmatrix}
=
\begin{pmatrix}
0 & 1 & 0 \\
-1 & 0 & 0 \\
0 & 0 & 0
\end{pmatrix}
$$

These matrix representation of a tensor is actually the tensor evaluated on the basis $(\hat{x}, \hat{y}, \hat{z})$.
So the multilinear function on a vector space that is a tensor can be visualized as a matrix when it is a rank 2 tensor
(it eats two vectors as input; keep reading, it'll make sense) when we are looking at just one component of
$x \times y$, that is, by comparing what we just learned with \ref{tensors:levi-civita-component-i} we can see that

$$
\left( x \times y \right)^i =
\begin{pmatrix}
    x_1, x_2, x_3 \\
\end{pmatrix}
\begin{pmatrix}
    \epsilon_{i11} & \epsilon_{i12} & \epsilon_{i13} \\
    \epsilon_{i21} & \epsilon_{i22} & \epsilon_{i23} \\
    \epsilon_{i31} & \epsilon_{i32} & \epsilon_{i33}
\end{pmatrix}
\begin{pmatrix}
    y_1 \\
    y_2 \\
    y_3
\end{pmatrix}
$$

And from here you should go and reach \ref{chapter:tensors}.
We'll wait for you here.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{On the Way to the Method of Stationary Phase: Method of Steepest Descent}

Building on top of the Laplace method, we now want to approximate
$$
\mathcal{I}(z) 
= \int_{C} f(z) e^{\lambda g(z)} dz
$$

Where $f(z)$ and $g(z)$ are analytical and we can deform the contour of integration
into any $C^\prime$ without changing the integral.
This is the complex analogue to the "fundamental theorem for gradients", where if the function whose gradient we are
integrating along a path is continuous, then its line integral along a close path is zero.

In complex analysis, this is the theorem called "Cauchy's integral theorem".
The theorem actually talks about holomorphic functions $f(z)$ being integrated on a simply connected
domain $\Omega$.
But since all holomorphic functions are analytical, this is the topic of Cauchy Riemman's Theorem,
and we can assume that we will always be on a simply connected domain (read Abbott's book on
analysis if you want to know more about that concept), then
$$
\int_{C} f(z) dz = 0
$$
\\


%%%%%%%%%%% Steepest descent argument.

Let's start this adventure by looking at the an analytic function $g(z) = X(z) + iY(z)$,
where $X(z)$ and $Y(z)$ are real-valued functions.


The steepest descent direction is the direction in which $\operatorname{Re}(g(z)) = X(z)$
decreases most rapidly, not $g(z)$ itself.
This is because we are interested in the behavior of the magnitude of the complex exponential
$e^{\lambda g(z)}$, which is governed by the real part of the exponent.
$$
e^{\lambda g(z)} = e^{\lambda (X(z) + iY(z))} = e^{\lambda X(z)} e^{i\lambda c}
$$

So steepest descent is given by the negative gradient of $X(z)$:

$$
-\nabla X = -\left(\frac{\partial X}{\partial x}, \frac{\partial X}{\partial y}\right)
$$


The gradient of $Y(z)$ is perpendicular to the level curves of $Y(z) = c$,
but we don't need to consider the negative gradient of $Y(z)$ for this proof.



Since $g(z)$ is analytic, the Cauchy-Riemann equations must hold, so that

$$
\frac{\partial X}{\partial x} = \frac{\partial Y}{\partial y}
$$
and
$$
\frac{\partial X}{\partial y} = -\frac{\partial Y}{\partial x}
$$


Now, let's use the Cauchy-Riemann equations to show that the constant phase contour is equivalent
to the steepest descent contour.
The direction of the constant phase contour is perpendicular to the gradient of $Y(z)$, so it is
some $\pi/2$ clockwise or counterclockwise.

A counterclockwise rotation of 90 degrees can be expressed as
$$
R_{90^\circ}
= \begin{pmatrix}
    0 &  -1 \\
    1 & 0
\end{pmatrix}
$$

Applying this rotation matrix to the gradient vector $\nabla Y$:
$$
\begin{pmatrix}
    0 & -1 \\[1em]
    1 & 0
\end{pmatrix}
\begin{pmatrix}
    \dfrac{\partial Y}{\partial x} \\[1em]
    \dfrac{\partial Y}{\partial y}
\end{pmatrix}
= \begin{pmatrix}
    -\dfrac{\partial Y}{\partial y} \\[1em]
    \dfrac{\partial Y}{\partial x}
\end{pmatrix}
$$

By the same logic we, clockwise rotation by 90 degrees would give us
$$
\begin{pmatrix}
    \dfrac{\partial Y}{\partial y} \\[1em]
    -\dfrac{\partial Y}{\partial x}
\end{pmatrix}
$$


Using the Cauchy-Riemann equations, we can rewrite the counterclockwise-rotated gradient of $Y(z)$ as:
$$
\left( -\frac{\partial Y}{\partial y}, \frac{\partial Y}{\partial x}\right)
= \left(-\frac{\partial X}{\partial x}, -\frac{\partial X}{\partial y}\right) = -\nabla X
$$
Notice that the minus sign points us in the direction of steepest descent,
so it is indeed the case that the \textbf{direction perpendicular to the gradient of $Y(z)$,
which is the direction of the constant phase contour,
is the same as the steepest descent direction, $-\nabla X$}.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Morse Theory: Intro}

Critical points can be degenerate or non-degenerate.
A non-degenerate critical point has a Hessian that can be inverted - the Hessian of the function $f$ does not vanish
at the critical point.

One associates a number called the \textbf{index}, the number of independent directions in which
$f$ decreases from a critical point.
More precisely, the index of a non-degenerate critical point $p$ of $f$ is the dimension of the largest subspace
of the tangent space to $M$ at $p$, where $M$ is a "landscape surface" function and $f : M \rightarrow \mathbb{R}$,
on which the Hessian of $f$ is negavite definite.

The \textbf{index} also corresponds to the number of negative eigenvalues of the Hessian matrix at the critical cpoint $p$.


In mathematics, a \textbf{symmetric matrix} $M$ with real entries is positive-definite if the real number $z^T M z$
is positive for every nonzero real column vector $z$.
More generally, a Hermitian matrix (that is, a complex matrix equal to its conjugate transpose) is \textbf{positive-definite}
if the real number $z^* M z$ is positive for every nonzero complex column vector $z$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Morse's Lemma}

This lemma is fundamental because it tells us that near a non-degenerate critical point, the function looks
like a quadratic form, and this form is determined by the signature of the Hessian matrix
(i.e., the number of positive and negative eigenvalues).

For example, suppose we have $f(x,y) = x^2 + y^2 - 2x + 4y$, $f : \mathbb{R}^2 \rightarrow \mathbb{R}$.

First, we find the critical point $\nabla f(x,y) = 0$,
\begin{align*}
\nabla f &= \left< \partial_x f, \partial_y f \right> \\
&= \left< 2x -2, 2y + 4 \right>
\end{align*}

and we set $\left( 2x -2, 2y + 4 \right) = \left( 0, 0 \right)$.
So we have $2x - 2 = 0$ or $x = 1$.
And then $2y + 4 = 0$, or $y = -2$.
So $p = (1, -2)$.

Now, let's see if the critical point is non-degenerate.
For that, let's compute the Hessian.
Remember that the Hessian is given by,
$$
H_f
=
\begin{pmatrix} 
    \dfrac{\partial^2 f}{\partial x_{1}^{2}}         & \dfrac{\partial^2 f}{\partial x_1 \partial x_2} & \dots  & \dfrac{\partial^2 f}{\partial x_1 \partial x_n}  \\[2.2ex]
    \dfrac{\partial^2 f}{\partial x_2 \partial x_1}  & \dfrac{\partial^2 f}{\partial x_{2}^{2}}        & \dots  & \dfrac{\partial^2 f}{\partial x_2 \partial x_n}  \\[2.2ex]
    \vdots                                           & \vdots                                          & \ddots & \vdots                                           \\[2.2ex]
    \dfrac{\partial^2 f}{\partial x_n \partial x_1}  & \dfrac{\partial^2 f}{\partial x_n \partial x_2} & \dots  & \dfrac{\partial^2 f}{\partial x_{n}^{2}}         \\
\end{pmatrix}
$$

That is, the entry of the $i$-th row and the $j$-th column is
$$
\left(H_f\right)_{i,j} = \frac{\partial^2 f}{\partial x_i \partial x_j}
$$

So we want to compute
\begin{align*}
H_f
&=
\begin{pmatrix} 
    \dfrac{\partial^2 f}{\partial x^{2}}         & \dfrac{\partial^2 f}{\partial x \partial y}  \\[2.2ex]
    \dfrac{\partial^2 f}{\partial y \partial x}  & \dfrac{\partial^2 f}{\partial y^{2}}         \\
\end{pmatrix} \\
&= \begin{pmatrix} 
    \partial_x \partial_x f  & \partial_x \partial_y f  \\[2.2ex]
    \partial_y \partial_x f  & \partial_y \partial_y f  \\
\end{pmatrix} \\
&= \begin{pmatrix} 
    \partial_x (2x -2)  & \partial_x (2y + 4)  \\[2.2ex]
    \partial_y (2x -2)  & \partial_y (2y + 4)  \\
\end{pmatrix} \\
&= \begin{pmatrix} 
    2  & 0  \\
    0  & 2  \\
\end{pmatrix}
\end{align*}

Given that all the entries on the diagonal are positive and there are no off-diagonal terms, it suggests that 
$f$ has a parabolic behavior in both the $x$ and $y$ directions independently
(recall the second derivative test; both diagonal entries are positive so the function is concave up),
which typically indicates a local minimum at the critical point if the function is convex or a saddle point if it is not. 

Anyway, the Hessian is invertible and its eigenvalues are positive, so $p$ is a minimum.
\\

Now we can apply Morse' lemma!
Since both eigenvalues of our Hessian are positive, then index, $k$, is 0 (no negative eigenvalues),
Morse's lemma tells us that near $p$ we can find coordinates $(u, v)$ such that
$$
f(u, v) = f(p) + u^2 + v^2 = -5 + u^2 +v^2
$$

So near the critical point, $f$ looks like a \textbf{paraboloid} downshifted by 5.

$$
f(x) = f(p) - x_{1}^{2} - \ldots - x_{\gamma}^{2} + x_{\gamma +1}^{2} + \ldots + x_{n}^{2}
$$

$\gamma$ is the index of $f$ at $p$.
The index $\gamma$ essentially determines how many negative squared terms appear in the local form of the function.
Our index was 0 so we didn't have any.

In Morse's lemma, when it says there exists a chart $(x_1, x_2, \ldots , x_n)$
it means that near the critical point $p$, you can introduce a new coordinate system where:

\begin{itemize}
    \item Each $x_i$ for $i = 1, \ldots , \gamma$ corresponds to a direction in which the function decreases
        (related to the negative eigenvalues of the Hessian).
    \item Each $x_i$ for $i = \gamma +1, \ldots, n$ corresponds to a direction in which the function increases
        (related to the positive eigenvalues of the Hessian).
\end{itemize}

The coordinates $x_i (p) = 0$ imply that the critical point $p$ is at the origin of this new coordinate system.
The transformation to this new coordinate system is achieved by a linear change of variables that diagonalizes
the Hessian matrix (through a process similar to eigendecomposition).

So here we added $u^2 + v^2$ because our Hessian matrix had 2 positive eignevalues.
\\~\\



The complex generalization as stated in
\href{https://en.wikipedia.org/wiki/Method_of_steepest_descent#Complex_Morse_lemma}{wikipedia: method of steepest descent}
provides another key idea of the Morse lemma.

As stated in the page,
if $z^0$ is a saddle point and $S(z)$ is a holomorphic function, then there exist coordinates in terms
of which $S(z) - S(z^0)$ is exactly quadratic.
We already saw above more or less how it looks for $S(Z) - S(Z^0)$, or $f(x) - f(p)$, to be exactly quadratic.
But the kicker is with the rest of the defintion.

If $S$ has a domain $W \in \mathbb{C}^n$, and $z^0$ is such that $\nabla S(z^0) = 0$ and
$S^{\prime\prime}_{zz} (z^0) \neq 0$.
Then there exists a neighborhood $U \in \mathbb{C}^n$ of $z^0$ and $V \in \mathbb{C}^n$ of $w=0$
such that a bijective holomorphic function can be defined

$$
\phi : V \rightarrow U
$$
and $\phi(0) = z^0$.
So this function sort of "remaps" our original function so that the saddle point becomes the origin.

The function is defined for all $w \in V$ as
$$
S(\phi(w))
=
S(Z^0) + \frac{1}{2} \sum_{j=1}^{n} \mu_j w_{j}^{2}
$$

and $\det(\phi^{\prime}_{w} (0)) = 1$.
Here $\mu_j$ are the eignevalues of $S^{\prime\prime}_{zz} (z^0)$.

The use of the complex morse lemma is that we can separate our initial problem into components.
Remember that we started with

$$
I(\lambda) =
\int_{C} f(z) e^{\lambda g(z)} dz
$$

The integral is broken into the domain with the saddle point and the one without it, and then expressed in terms of
the bijective function $\phi$, that maps a domain around $z^0$ to some other domain $U$.
So we in the limit of $\lambda \rightarrow \infty$

\begin{align*}
I(\lambda) &= I_0 (\lambda) =
\int_{C_w} f(\phi(w)) e^{\lambda S(\phi(w))} \, d\left(\phi (w)\right) \\
&= \int_{C_w} f(\phi(w)) \left( e^{\lambda S(z^0)} e^{\frac{1}{2} \lambda \sum_j \mu_j w_{j}^{2}} \right) \, d\left(\phi (w)\right) \\
&= e^{\lambda S(z^0)} \int_{C_w} f(\phi(w)) \exp\left( \frac{1}{2} \lambda \sum_j \mu_j w_{j}^{2} \right) \, \Big| \det \phi^{\prime}_{w}\Big| dw
\end{align*}

To simplify the expression we have to remember how to use the Jacobian to fully do our transformation of variables.
\\

$$
f(z) = \int_{0}^{1} \frac{d}{dt} f(tz_1 , \dots , tz_n) dt
= 
\sum_{i=1}^{n} z_i \int_{0}^{1} \frac{\partial f(z)}{\partial z_i} \Big|_{z = (tz_1, \dots , tz_n)} dt
$$

If we apply the chain rule, while keeping in mind that $z_i$ is the actual variable of $f$,
not $tz$.

$$
\frac{d}{dt} f(tz_1, ..., tz_n)
=
\sum_{i=1}^{n} \frac{\partial f}{\partial z_i} \cdot \frac{d}{dt}(tz_i)
=
\sum_{i=1}^{n} z_i \frac{\partial f}{\partial z_i}
$$
\\


$$
\mathcal{I}_j
=
\int_{-\infty}^{\infty} e^{\frac{1}{2}\lambda_j y^2} dy
=
2 \int_{0}^{\infty} e^{-\frac{1}{2}\lambda\left(\sqrt{-\mu_j} y\right)^2} dy
=
2 \int_{0}^{\infty} e^{-\frac{1}{2}\lambda |-\mu_j|^2 y^2 \exp(2i \arg \sqrt{-\mu_j})} dy
$$

The last expression is obtained by writing the complex number $\mu$ as
$$
-\mu_j = |-\mu_j| e^{i \arg(-\mu_j)}
$$
or
$$
\sqrt{-\mu_j} = \sqrt{|-\mu_j|} e^{i \arg(\sqrt{-\mu_j})}
$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Homotopy}

If $M$ is a "landscape surface" function and $f : M \rightarrow \mathbb{R}$,
and $M^a = f^{-1} (-\infty, a]$,
The topology of $M^{a}$ does not change except when $a$ passes the height of a critical point;
at this point, a $\gamma$-cell is attached to $M^{a}$, where $\gamma$ is the index of the point.
This does not address what happens when two critical points are at the same height,
which can be resolved by a slight perturbation of $f$.
In the case of a landscape or a manifold embedded in Euclidean space, this perturbation might simply be tilting slightly, rotating the coordinate system. 