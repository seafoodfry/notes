\section{Integrals in QFT}

\subsection{Kinetic operator and a Green Function} \label{integral:kinetic-operator}

This problem is part of
\href{https://arxiv.org/abs/physics/0609191}{MontePython: Implementing Quantum Monte Carlo using Python}.

There is a kinetic operator defined as
$$
G_K \left(\mathbf{R}^\prime, \mathbf{R}, t\right) =
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-i\mathbf{k}\mathbf{R}^\prime} e^{-Dtk^2} e^{-i\mathbf{k}\mathbf{R}} d\mathbf{k}
$$

This integral then is integrated and the following Green function is obtained
$$
G_K \left(\mathbf{R}^\prime, \mathbf{R}, t\right) =
\frac{1}{\left(4\pi Dt\right)^{3N/2}} e^{-\left(\mathbf{R}-\mathbf{R}^\prime\right) / 4Dt}
$$

And as we saw in \hyperref[jackson:problem-1.2]{Jackson problem 1.2}, the Green function we just got is equivalent to
$\delta\left(\mathbf{R} - \mathbf{R}^\prime\right)$.

But coming back to the integral, the trick is to complete the square!
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-i\vect{k}\vect{R}^\prime} e^{-Dtk^2} e^{-i\vect{k}\vect{R}} d\vect{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dtk^2 + -i\vect{k}\cdot\left(\vect{R}-\vect{R}^\prime\right)} d\mathbf{k}
\end{align*}

This last integral has the form $\int e^{ax^2 + bx} dx$, which can be solved as follows,
\begin{align*}
\int e^{ax^2 + bx} dx &= \int \exp\left\{ a\left(x^2 + \frac{b}{a}x\right) \right\} dx \\
&= \int \exp\left\{ a\left(x^2 + \frac{b}{a}x + \left(\frac{b}{2a}\right)^2 - \left(\frac{b}{2a}\right)^2 \right) \right\} dx \\
&= \int \exp\left\{ a\left(x + \frac{b}{2a}\right)^2 - \frac{b^2}{4a} \right\} dx \\
&= e^{-b^2/4a} \int e^{ a\left(x + \frac{b}{2a}\right)^2 } dx
\end{align*}

Now, let's look at $\int e^{ a\left(x + b/2a\right)^2 } dx$.
First, we need to make a transformation.
Let's define $u = x + b/2a$, then $du = dx$, the measure remains invariant (so do the limits of integration), so
$\int e^{ a\left(x + b/2a\right)^2 } dx = \int e^{ au^2 } du$.
Now, if we define $a = -c$, then 

\begin{align*}
\int e^{ a\left(x + b/2a\right)^2 } dx &= \int e^{ au^2 } du \\
&= \int e^{ -cu^2 } du \\
&= \sqrt{ \frac{\pi}{c} } = \sqrt{ \frac{\pi}{-a} } 
\end{align*}

This whole thing works if $a < 0$ but it turns out that this is also valid if $\re{(a)} \leq 0$ but $a \neq 0$.

Putting everything back together,
$$
\int e^{ax^2 + bx} dx = \sqrt{ \frac{\pi}{-a} } e^{-b^2/4a}
$$

Looking back at our original problem, we can chose $a = -Dt$ and $b = i \vect{r}$, where $\vect{r} = \vect{R}-\vect{R}^\prime$.
and so
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dtk^2 + -i\vect{k}\cdot\vect{r}} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k^2 + \frac{i\vect{k}}{Dt}\cdot\vect{r} \right)} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k^2 + \frac{i\vect{r}}{Dt}\cdot\vect{k} + \left(\frac{i\vect{r}}{2Dt}\right)^2 - \left(\frac{i\vect{r}}{2Dt}\right)^2 \right)} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 - \frac{ \vect{r}^2 }{ 4Dt } } d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } d\mathbf{k} \\
\end{align*}

Again, since $\int e^{-ax^2} dx = \sqrt{ \frac{\pi}{a} }$,
then
\begin{align*}
\int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } dk &= \sqrt{ \frac{\pi}{Dt} }
\end{align*}

Thus,
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \left( \sqrt{ \frac{\pi}{Dt} } \right)^{3N} \\
&= \frac{1}{\left(4\pi^2\right)^{3N/2}} \frac{\pi^{3N/2}}{(Dt)^{3N/2}} e^{-\vect{r}^2 /4Dt } \\
&= \frac{1}{\left(4\pi Dt\right)^{3N/2}} e^{-\left(\vect{R} - \vect{R}^\prime\right)^2 /4Dt }
\end{align*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Klein-gordon Non-relativistic Amplitude} \label{ps-kg:nonrelativistic-propagator}

The following integral comes from Peskin and schroeder Section 2.1
\begin{align}
& \int \frac{d^3p}{(2\pi)^3} \, e^{-i(\vect{p}^2/2m)t} e^{i\vect{p}\cdot(\vect{x}-\vect{x}_0)} \label{kg:nonrelativistic-03} \\
&= \left( \frac{m}{2\pi i t} \right)^{3/2} e^{im (\vect{x}-\vect{x}_0)^2 /2t} \label{kg:nonrelativistic-04}
\end{align}

This integral follows the same procedure as \ref{integral:kinetic-operator}.

Here we have $a = -\frac{it}{2m}$ and $b = i(\vect{x}-\vect{x_0})$.
Since $\re{a} = -t/2m \leq 0$ and $a\neq 0$ when $t\neq 0$ oru solution is valid for $t\neq 0$.
Then

$$
\sqrt{ \frac{\pi}{-a} } = \sqrt{ \frac{2\pi m}{it} }
$$

and
\begin{align*}
-\frac{b^2}{4a} &= \frac{2m \left(\vect{x}-\vect{x}_0\right)^2}{-4it} \\
&= \frac{im \left(\vect{x}-\vect{x}_0\right)^2}{2t}
\end{align*}

So the solution is
\begin{align*}
& \frac{1}{(2\pi)^3} \left( \frac{2\pi m}{it} \right)^{3/2} e^{\frac{im \left(\vect{x}-\vect{x}_0\right)^2}{2t}} \\
&= \left( \frac{m}{2\pi it} \right)^{3/2} e^{im \left(\vect{x}-\vect{x}_0\right)^2 /2t}
\end{align*}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tensors}

Working with a spacetime metric signature $(+ - - -)$,
$$
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
$$

$x^\mu = (x^0, \vect{x})$ and $x_\mu = g_{\mu\nu} x^\nu = (x^0, -\vect{x})$.
To see this explciitly, let's carry out the summations.

When $\mu = 0$, $x_0 = g_{00}x^0 + g_{01}x^1 + g_{02}x^2 + g_{03}x^3 = x^0 + 0 + 0 +0 = x^0$.
For $\mu = 1$, $x_1 = g_{10}x^0 + g_{11}x^1 + g_{12}x^2 + g_{13}x^3 = 0 + x^1 + 0 +0 = -x^0$.
Similarly, when $\mu = 2$, $x_2 = -x^2$, and when $\mu =3$, $x_3 = -x^3$.

The contravariant components of the vector $x^\mu$ are just its components.
The covariant compoents $x_\mu$, defined to be equal to $x_\mu = x^\nu g_{\nu\mu}$, are actually the components
of the associated dual vector.
Why? well, in order to obtain $x_\mu$ we had to use a "non-degenerate hermitian form"
(something that looks like an inner product; though it isn't since this metric is not positive definite).
This makes it a dual vector by definition.
\\

Also note that $g_{\mu\nu}$ forms a \textbf{bilinear form} since $g(x_1, x_2) = g(x_2, x_1)$
(linearity on both sides of the non-degenerate hermitian form).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inner Product}

Why does $p \cdot x = g_{\mu\nu} p^\mu x^\nu = g^{\mu\nu} p_\mu x_\nu = p_\mu x^\nu = p^\mu x_\nu = p^0 x^0 - \vect{p}\cdot\vect{x}$?

To start, we ought to know that there is the implicit defintion of
$x_\mu = x^\nu g_{\nu\mu}$.
So the covariant vector is defined through the application of a non-degenerate hermitian form
and it constitutes a dual vector.
From there the rest is linear algebra.

The metric tensor defines the geometry of spacetime, including the way distances and angles are measured.
The implied summation, einstein summation, effectively 'weights' the components according to the geometry of spacetime.

$g_{\mu\nu}$ lowers indices, while $g^{\mu\nu}$ raises them.
$\delta^{\mu}_{\nu}$ is a diagonal matrix with ones on the diagonal and zeros elsewhere.
It selects the $\mu$-th component when used in a summation, acting like an identity element.

$g^{\mu\alpha} g_{\alpha\nu} = \delta^{\mu}_{\nu}$

$$
g^{\mu\alpha} g_{\alpha\nu} = \sum_{\alpha = 0} g^{\mu\alpha} g_{\alpha\nu} =
g^{\mu 0} g_{0 \nu} + g^{\mu 1} g_{1\nu} + g^{\mu 2} g_{2\nu} + g^{\mu 3} g_{3\nu}
$$

This is essentially a matrix multiplication.
$$
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
\cdot
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
=
\begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
\end{pmatrix}
$$

Going component by component, take the second row and second column, $\mu=1$ and $\nu=1$.
The element for that entry is given by
$$
g^{1\alpha}g_{\alpha 1} =
g^{1 0} g_{0 1} + g^{1 1} g_{1 1} + g^{1 2} g_{2 1} + g^{1 3} g_{3 1}
= 0 + (-1)(-1) + 0 + 0 = 1
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Derivatives}

The examples here come from \cite{tensor-calc}.
The aim here is to see a few examples to get used to the quantities we are going to be seeing later on.
\\

Some useful identities before we carry on:
\begin{enumerate}
    \item
    $$
    \frac{\partial x_p}{\partial x_q} = \delta_{pq}
    $$
\end{enumerate}



\textbf{Example 1}

If $a_{ij}$ are contants, to compute the partial derivative
$$
\frac{\partial}{\partial x_k} a_{ij} x_i x_j
$$

We'll firs write out the sums explciitly.
\begin{align*}
a_{ij} x_i x_j &= \sum_{i,j} a_{ij} x_i x_j
\end{align*}

Someone once happened to noticed that the sum could be broken into parts depending on when $x_{\{i,j\}} = x_k$, where $x_k$
is the one we want to differentiate with respect to,
\begin{align*}
\sum_{i,j} a_{ij} x_i x_j &=
    \sum_{i \neq k, j \neq k} a_{ij} x_i x_j +
    \sum_{i, j \neq k} a_{ij} x_i x_j + 
    \sum_{i \neq k, j} a_{ij} x_i x_j +
    \sum_{i=k, j=k} a_{ij} x_i x_j \\
&= C + \sum_{j \neq k} \left( a_{kj} x_j \right) x_k +
    \sum_{i \neq k} \left( a_{ik} x_i \right) x_k +
    a_{kk} x_k x_k \\
    &= C + \sum_{j \neq k} \left( a_{kj} x_j \right) x_k +
    \sum_{i \neq k} \left( a_{ik} x_i \right) x_k +
    a_{kk} (x_k)^2 \\
\end{align*}

We wrote the first sum, where no $x$s are equal to $x_k$ as $C$, because as you may guess this is going to go away with
any derivative.

Back to our original task,
\begin{align*}
\frac{\partial}{\partial x_k} a_{ij} x_i x_j &=
    0 + \sum_{j \neq k} a_{kj} x_j +
    \sum_{i \neq k} a_{ik} x_i +
    2 a_{kk} x_k \\
&= \sum_{j} a_{kj} x_j + \sum_{i} a_{ik} x_i \\
&= a_{kj} x_j + a_{ik} x_i \\
&= a_{ki} x_i + a_{ik} x_i \\
&= (a_{ik} + a_{ki}) x_i
\end{align*}



\textbf{Example 2}

Another way to work out
$$
\frac{\partial}{\partial x_k} a_{ij} x_i x_j
$$
goes as follows.

\begin{align*}
\frac{\partial}{\partial x_k} a_{ij} x_i x_j &= a_{ij} \frac{\partial}{\partial x_k} x_i x_j \\
&= a_{ij} \left( \frac{\partial x_i}{\partial x_k} x_j + x_i \frac{\partial x_j}{\partial x_k} \right) \\
&= a_{ij} \left( \delta_{ik} x_j + x_i \delta_{jk} \right) \\
&= a_{kj} x_j + a_{ik} x_i \\
&= (a_{ik} + a_{ki}) x_i
\end{align*}


\textbf{Example 3}

If $a_{ij} = a_{ji}$ are constants, compute
$$
\frac{\partial^2}{\partial x_k \partial x_l} a_{ij} x_i x_j
$$

Let's start with our previous result,
\begin{align*}
\frac{\partial^2}{\partial x_k \partial x_l} a_{ij} x_i x_j &=
    \frac{\partial}{\partial x_k} \left[ \frac{\partial}{\partial x_l} a_{ij} x_i x_j \right] \\
&= \frac{\partial}{\partial x_k} (a_{il} + a_{li}) x_i \\
&= 2 a_{li} \frac{\partial x_i}{\partial x_k} \\
&= 2 a_{li} \delta_{ik} \\
&= 2 a_{lk}
\end{align*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Changing Variables: the Jacobian}


Promise we are not doing the conventional thing where we go from scratch but there is one more pattern we have to covered
now in order for some other equations to make sense and this is the Jacobian.


There is a result from calculus that is presented a bit like this (the following presentation comes from
\cite{jacobian}):


If we define the Jacobian of a transformation $x = g(u,v)$, $y = h(u,v)$ as
\begin{align*}
\frac{ \partial (x,y) }{ \partial (u,v) } &= 
\begin{vmatrix}
\dfrac{ \partial x }{ \partial u } & \dfrac{ \partial x }{ \partial v } \\ \addlinespace
\dfrac{ \partial y }{ \partial u } & \dfrac{ \partial y }{ \partial v }
\end{vmatrix} \\
&= \frac{ \partial x }{ \partial u } \frac{ \partial y }{ \partial v } -
    \frac{ \partial x }{ \partial v }\frac{ \partial y }{ \partial u }
\end{align*}

Then if we want to integrate $f(x)$ over some surface $R$, this region will become some other surface $S$
following our transformation and the integral can be computed as follows:
$$
\int \int_{R} f(x,y) dA
= \int \int_{S} f\left( g(u,v), h(u,v) \right) \Biggl| \frac{ \partial (x,y) }{ \partial (u,v) } \Biggl| d\overline{A}
$$

For example, when we go from Euclidean to polar coordinates, the transformation we use is $x = r\cos\theta$
and $y = r\sin\theta$ and the Jacobian is $r$ so
$dA = \left| \frac{\partial (x, y)}{\partial (r, \theta)} \right| dx dy = r dr d\theta$:

\begin{align*}
\frac{\partial (x, y)}{\partial (r, \theta)} &= 
\begin{vmatrix}
    \dfrac{ \partial x }{ \partial r } & \dfrac{ \partial x }{ \partial \theta } \\ \addlinespace
    \dfrac{ \partial y }{ \partial r } & \dfrac{ \partial y }{ \partial \theta }
\end{vmatrix} \\
& = \begin{vmatrix}
    \cos\theta & -r\sin\theta \\ \addlinespace
    \sin\theta & r\cos\theta
\end{vmatrix} \\
&= r \cos^2 \theta + r \sin^2 \theta \\
&= r
\end{align*}



In 3-dimensions this extends to
$$
\int \int \int_{V} f(x,y,z) dV
= \int \int \int_{B} f\left( g(u,v,w), h(u,v,w), k(u,v,w) \right) \Biggl| \frac{ \partial (x,y,z) }{ \partial (u,v,w) } \Biggl| d\overline{V}
$$

For the case of the transformation of Euclidean coordinates to spherical coordinates, the transformation of variables is
$x = r \sin\phi \cos\theta $, $y = r \sin\phi \sin\theta$, and $z = r\cos\phi$.
In that case the Jacobian is $r^2 \sin\phi$ and thus
$dV = \left| \frac{\partial (x, y, z)}{\partial (r, \theta, \phi)} \right| dx dy dz = r^2 \sin\phi dr d\phi d\theta$.


Besides being very widely used, the reason we mentioned it is because this is also that is very well used in
\cite{gifted-qft} to introduce the notation we see so much in QFT.
Specifically, the Jacobian as we have been writing it, can also be seen in expressions such as

$$
\overline{a}^{\mu} = \left( \frac{\partial \overline{x}^\mu}{\partial x^\nu} \right) a^\nu
$$

$$
\frac{\partial \phi}{\partial \overline{x}^\mu} =
    \left( \frac{\partial x^\nu}{\partial \overline{x}^\mu} \right) \frac{\partial \phi}{\partial x^\nu}
$$

The determinant of the Jacobian matrix is used specifically when you are dealing with volume transformations
(like integrating over a volume or changing the measure of integration) because it scales the volume elements according
to how the coordinate transformation stretches or compresses the space.

When transforming vectors, however, you're not scaling a volume but rather reorienting or rescaling each component
of the vector according to how the coordinate axes themselves have changed.
This is why the transformation involves a sum over the product of vector components and the appropriate Jacobian elements:

\begin{itemize}
\item Contravariant vectors (standard vectors): The components of these vectors are transformed by multiplying with the
    Jacobian matrix $\partial \overline{x}^\mu / \partial x^\nu$. Here, each new vector component is a linear combination
    of old components weighted by how much each new coordinate axis changes with respect to each old axis.
    \begin{itemize}
        \item These objects, such as displacement vectors, inherently "follow" the coordinate axes. As the axes stretch
        or rotate, so do the components of the vector. 
    \end{itemize}
\item Covariant vectors (gradients and one-forms): These vectors transform using the inverse of the Jacobian matrix
    $\partial x^\nu / \partial \overline{x}^\mu$. This reflects how changes in the old coordinates map to the new
    coordinates, suitable for objects that naturally pair with vectors, like gradients.
    \begin{itemize}
        \item They measure rates of change along these coordinates.
        \item A larger coordinate stretch means a smaller gradient in that direction to maintain the same rate of change.
    \end{itemize}
\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{From Tensors to Matrices}

Tensors can be expressed as matrices only once a basis has been chosen.
the example being given is with the angular momentum operator $L_z$ on the spherical harmonics $H_l (\mathbb{R}^3)$.

It says that $H_l (\mathbb{R}^3)$ is the set of all linear functions on $\mathbb{R}^3$ that

$$
\{ r Y_{m}^{l} \}_{ -1 \leq m \leq 1 } =
\Big\{ \frac{1}{\sqrt{2}} (x +iy), \, z, \, - \frac{1}{\sqrt{2}} (x -iy) \Big\}
$$

and $\{ x, y, z \}$ are both basis for the space.

Then having the angular momentum operator $L_z = -i ( x\partial_y - y \partial_x )$ on this space we got
$$
\frac{1}{\sqrt{2}} L_z (x + iy) = \frac{1}{\sqrt{2}} (x + iy)
$$
$$
L_z (z) = 0
$$
$$
-\frac{1}{\sqrt{2}} L_z (x -iy) = \frac{1}{\sqrt{2}} (x -iy)
$$

which implies that in the spherical harmonics basis

$$
L_z = 
\begin{pmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & -1
\end{pmatrix}
$$

Could you tell me how that tensor representation of $L_z$ is implies by saying that
$T(e_i) = \sum^{n}_{j=1} T_{i}^{j} e_j$ ?


$$
\frac{1}{\sqrt{2}} L_z (x + iy)
\rightarrow
L_z \left( \frac{1}{\sqrt{2}} (x + iy) \right)
$$

$$
L_z (z) = 0
$$

$$
-\frac{1}{\sqrt{2}} L_z (x -iy)
\rightarrow
L_z \left( -\frac{1}{\sqrt{2}} (x + iy) \right)
$$

Since

$$
\{ r Y_{m}^{l} \}_{ -1 \leq m \leq 1 } =
\Big\{ \frac{1}{\sqrt{2}} (x +iy), \, z, \, - \frac{1}{\sqrt{2}} (x -iy) \Big\}
$$

then we can say that $e_1 = \frac{1}{\sqrt{2}} (x +iy)$, $e_2 = z$, and $e_3 = -\frac{1}{\sqrt{2}} (x -iy)$.

Meaning that $L_z (e_1) = e_1 = 1\cdot e_1 + 0\cdot e_2 + 0\cdot e_3$,
$L_z (e_2) = 0 = 0\cdot e_1 + 0\cdot e_2 + 0\cdot e_3$, and
$l_z (e_3) = -e_3 = 0\cdot e_1 + 0\cdot e_2 - 1\cdot e_3$.
From here, we can then adopt write this as a matrix

$$
T_{i}^{j}
=
L_z
=
\begin{pmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & -1
\end{pmatrix}
$$

The columns of a matrix representation of a linear operator correspond to the action of the operator on the basis vectors.
However, due to the specific diagonal form of the matrix in this example, both the row and column perspectives
give the same result, which can indeed be confusing.

The first column represents $L_z(e_1)$,
the second column represents $L_z(e_2)$,
the third column represents $L_z(e_3)$.

In physics speak this would be read as the wave functions
$\frac{1}{\sqrt{2}} (x +iy)$, $z$, and $- \frac{1}{\sqrt{2}} (x -iy)$
have eigenvalues 1, 0, and -1, respectively.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Now Let's Talk Tensors}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Transformation Matrices}

The previous section gave us a good place to begin talking about tensors.
Now let's actually talk about them.

Tensors have been historically defined based on how they transform.
Think of the conventional axis rotation formula in 2D.
If we rotate the axis by an angle $\theta$, then

$$
x^\prime = x \cos\theta - y \sin\theta
$$
and
$$
y^\prime = x \sin\theta + y \cos\theta
$$

As we saw in the previous section,
where we derived the matrix representation of the angular momentum operator,
the columns of a matrix representation of a multilinear map (tensor) correspond to the action of
the map (tensor) on the basis vectors.
Which is most commonly seen as

$$
\begin{pmatrix}
x^\prime \\
y^\prime
\end{pmatrix}
=
\begin{pmatrix}
\cos\theta & - \sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix}
$$

If we envision $x$ and $y$, and $x^\prime$ and $y^\prime$, and some basis sets,
such that $x \rightarrow e_1$ and $y \rightarrow e_2$ (similarly for the primed components).
We can rewrite the above as
$$
e_{1^\prime} = e_1 \cos\theta - e_2 \sin\theta
$$
and
$$
e_{2^\prime} = e_1 \sin\theta + e_2 \cos\theta
$$

Meaning that our problem becomes,
$$
\begin{pmatrix}
e_{1^\prime} \\
e_{2^\prime}
\end{pmatrix}
=
\begin{pmatrix}
\cos\theta & - \sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix}
\begin{pmatrix}
e_1 \\
e_2
\end{pmatrix}
$$

Following the Einstein summation convention, we could write

$$
e_{i^\prime} = A_{i^\prime}^{j} e_j
$$

which also means that the original basis can be obtained as follows
$$
e_{i} = A_{i}^{j^\prime} e_{j^\prime}
$$

It is interesting to now quickly pursue the exercise of expressing our basis in terms of itself
as it will show us some properties about these numbers $A_{i^\prime}^{j}$ and $A_{i}^{j^\prime}$.


Combining the previous two expressions we get,
$$
e_{i} = A_{i}^{j^\prime} e_{j^\prime}
= A_{i}^{j^\prime} A_{j^\prime}^{k} e_k
$$

From here we can see that
$$
A_{i}^{j^\prime} A_{j^\prime}^{k} = \delta_{i}^{k}
$$

If we consider the same example but with the primed-basis.

$$
e_{i^\prime} = A_{i^\prime}^{j} e_j 
= A_{i^\prime}^{j} A_{j}^{k^\prime} e_{k^\prime}
$$

Meaning that

$$
A_{i^\prime}^{j} A_{j}^{k^\prime} = \delta_{i^\prime}^{k^\prime}
$$

So the different $A$s are inverse of one another.

It is also worth noting that these $A$s are tensor components.


In expressions such as $A_{i^{\prime}}^{j} A_{j}^{k^{\prime}}$, the indices $i^{\prime}$ and $k^{\prime}$ refer
to the primed basis, while the index $j$ refers to the unprimed basis.
This mixing of basis sets in a single expression is why the numbers $A_{i^{\prime}}^{j}$ and $A_{j}^{k^{\prime}}$
are not considered tensor components.

The transformation matrices $A_{i^{\prime}}^{j}$ and $A_{j}^{k^{\prime}}$ are not tensor components themselves,
but rather express the relationship between components in different bases.
The fact that these matrices are inverses of each other, which is a consequence of the transformation law for
tensor components.

Another useful thing to note is that the transformation for dual vectors looks the same but with uppoer indices.

$$
e^{i^\prime} = A_{j}^{i^\prime} e^j
$$
and
$$
e^{i} = A_{j^\prime}^{i} e^{j^\prime}
$$

With this info at hand, we can guesstimate the matrix transformation laws for tensors,

$$
A =
\begin{pmatrix}
A_{1}^{1^\prime} & A_{2}^{1^\prime} & \dots  & A_{n}^{1^\prime} \\
A_{1}^{2^\prime} & A_{2}^{2^\prime} & \dots  & A_{n}^{2^\prime} \\
\vdots           & \vdots           & \ddots & \vdots           \\
A_{1}^{n^\prime} & A_{2}^{n^\prime} & \dots  & A_{n}^{n^\prime}
\end{pmatrix}
$$

Its inverse is
$$
A^{-1}
=
\begin{pmatrix}
A_{1^\prime}^{1} & A_{2^\prime}^{1} & \dots  & A_{n^\prime}^{1} \\
A_{1^\prime}^{2} & A_{2^\prime}^{2} & \dots  & A_{n^\prime}^{2} \\
\vdots           & \vdots           & \ddots & \vdots           \\
A_{1^\prime}^{n} & A_{2^\prime}^{n} & \dots  & A_{n^\prime}^{n} \\
\end{pmatrix}
$$

These are unitary matrices
$$
A A^{-1} = A^{-1} A = I
$$

Equating the above with our 2D rotation example,
$$
A^{-1}
=
\begin{pmatrix}
\cos\theta & - \sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix}
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Contravariant and Covariant Vectors}

One fun fact about vectors is that they can be seen as tensors of type (0,1), that is, they feed
on 1 dual vector.
A vector can be viewed as a tensor of type (0, 1) because it transforms according to the contravariant
transformation law under a change of coordinates. The key idea is that a vector represents a directed
quantity that "eats" or operates on dual vectors (covectors or linear functionals) to produce scalars.

This is mathematically said as,
$$
v^{i^\prime} = A^{i^\prime}_{j} v^j
$$
As we saw above, this is the transformation for dual vectors - expressions with contravariant indices.

And as we saw above, these transform according to the Jacobian.

A dual vector in turn transforms as
$$
f_{i^\prime} = A_{i^\prime}^{j} v_j
$$

Note how the transformation are the contrary to what we saw for the basis vectors!
Where as the components of the dual vector transform in the same way as the basis.

In matrix notation we have contravariant vectors transform as
$$
v_{\mathcal{B}^\prime} = A v_{\mathcal{B}}
$$

and covariant ones as
$$
f_{\mathcal{B}^\prime} = \left( A^{-1} \right)^T f_{\mathcal{B}}
$$

But remember that basis and dual basis transform in the opposite ways!
\\


Now, in quantum mechanics, if we consider an orthonormal basis ${\ket{e_{i^{\prime}}}}$ in the same Hilbert space.
And assume that both ${\ket{e_i}}$ and ${\ket{e_{i^{\prime}}}}$ are complete bases,
so there must be a unitary transformation $U$ that relates them.
We would write, $\ket{e_{i^{\prime}}} = U \ket{e_i}$ for this.
Consecuently, the dual basis transforms as follows, $\bra{ e_{i^\prime} } = \bra{e_i} U^\dagger $.

Here, $U$ plays the role of $A^{-1}$, or $A_{i^{\prime}}^{j}$ in the tensor formalism.

Following the previous approach, we get the following results for transformations into a primed basis set,
$$
\ket{e_{i^{\prime}}} = U \ket{e_i}
$$
and
$$
\bra{ e_{i^\prime} } = \bra{e_i} U^\dagger
$$

The matrix elements of $U$ in the $\ket{e_i}$ basis are given by:
$U_{i^{\prime}}^{j} = \bra{e_{i^{\prime}}} U \ket{e_j}$.
These matrix elements play the same role as the $A_{i^{\prime}}^{j}$ in your classical expression.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pauli Matrices}

The Pauli matrices actually form a basis for $2 \times 2$ Hermitian matrices.
Let's see how: a generic $2 \times 2$ Hermitian matrix would look like

$$
\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}
=
\begin{pmatrix}
a^* & c^* \\
b^* & d^*
\end{pmatrix}
$$

Meaning that $a, b \in \mathbb{R}$ and that $b = c^*$.
So we could write our Hermitian matrix as

$$
\begin{pmatrix}
a      & x - iy \\
x + iy & d
\end{pmatrix}
$$

Another requirement that is imposed on $H_2 (\mathbb{C})$ is that they must be unitary, that is $U^\dagger U = I$, so

\begin{align*}
\begin{pmatrix}
a      & x - iy \\
x + iy & d
\end{pmatrix}
\begin{pmatrix}
a      & x - iy \\
x + iy & d
\end{pmatrix}
&=
\begin{pmatrix}
a^2 + x^2 - y^2         & a(x - iy) + b(x - iy) \\
a(x + iy) + b(x + iy) & d^2 + x^2 - y^2
\end{pmatrix}
\\
&=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\end{align*}

One way to make this happen is to set $a = -d$, this will also have the happy side effect of meeting another requirement
for the Pauli matrices to be traceless.
So now we have our candidate bases that ought to look like

$$
\begin{pmatrix}
a      & x - iy \\
x + iy & -a
\end{pmatrix}
$$

This then means that our requirement for our matrix $H_2 (\mathbb{C})$ to be unitary simplifies to
$$
\begin{pmatrix}
a^2 + x^2 - y^2 & 0                \\
0               & a^2 + x^2 - y^2
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
$$

Here is where our 3 Pauli matrices comes!

$$
\sigma_1 = \sigma_x =  
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
$$
comes from assuming that $x=1$, so $a^2 + x^2 - y^2 = 1$ can be met by assuming that $a = y = 0$.

$$
\sigma_2 = \sigma_y =  
\begin{pmatrix}
0 & -i \\
i & 0
\end{pmatrix}
$$

comes from assuming that $y=1$, so $a^2 + x^2 - y^2 = 1$ can be met by assuming that $a = x = 0$.

Finally,
$$
\sigma_3 = \sigma_z =  
\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
$$

Where we took $a=1$ and $x = y = 0$.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Frobenius Method}

\subsection{A Generic Example}

The following example comes from
\href{https://en.wikipedia.org/wiki/Frobenius_method}{wikipedia: frobenius method}:

$$
z^2 f^{\prime\prime} - zf^\prime + (1-z)f = 0
$$

We assume the solution has the form,
$$
f = \sum_{k=0} A_k z^{k+r}
$$

Plugging the above series into the ODE and organizing like terms we land in
$$
\sum_k \left[ (k+r)(k+r-1) - (k+r) + (1-z) \right] A_k z^{k+r} = 0
$$
which can be further simplified to

$$
\sum_k \left[ (k+r)(k+r-1) - (k+r) + 1 \right] A_k z^{k+r}
    - \sum_k A_k z^{k+r+1} 
= 0
$$

The indicial polynomial can then be obtained by considering the lowest power of $z$, which is $z^r$,
$$
(r(r-1) + 1) A_0 z^r
$$

We now set $z=0$ and obtain
$$
r(r-1) + 1 = r^2 -r + 1 = (r-1)^2 = 0
$$

Meaning that $r=1$ (double root).

We now need to find the recurrence relation by plugging $f = \sum_{k=0} A_k z^{k+1}$ back into our original proble,
equate coefficients of like powers of $z$, and find a recurrence relation for $A_k$.

We now have,
$$
f^\prime = \sum_{k=0} A_k (k+1) z^k
$$
and
$$
f^{\prime\prime} = \sum_{k=0} A_k k(k+1) z^{k-1}
$$

And our ODE now becomes,
\begin{align*}
& z^2 \sum_{k} A_k k(k+1) z^{k-1}
    - z \sum_{k} A_k (k+1) z^k
    + (1-z) \sum_{k} A_k z^{k+1} \\
&= \sum_{k} A_k k(k+1) z^{k+1}
    - \sum_{k} A_k (k+1) z^{k+1}
    + \sum_{k} A_k z^{k+1}
    - \sum_{k} A_k z^{k+2} \\
&= \sum_k \left[ k(k+1) - (k+1) + 1 \right] A_k z^{k+1} - \sum_{k} A_k z^{k+2} \\
&= \sum_k k^2 A_k z^{k+1} - \sum_{k} A_k z^{k+2}
\end{align*}

To find a recurrence relation we need to align the sums.
So we can write
\begin{align*}
& \sum_{k=0} k^2 A_k z^{k+1} - \sum_{k=0} A_k z^{k+2} \\
&= \sum_{k=0} k^2 A_k z^{k+1} - \sum_{k=1} A_{k-1} z^{k+1}
\end{align*}

We could do a similar shift for the first sum but since the first term $k=0$ has a $k^2$ term then we can ignore it.
Thus,
$$
\sum_{k=1} \left( k^2 A_k - A_{k-1} \right) z^{k+1} = 0
$$

Since this sum must be zero for all $z$, the coefficients must satisfy,
$$
k^2 A_k - A_{k-1} = 0
$$
or
$$
A_k = \frac{ A_{k-1} }{k^2} 
$$

This recurrence realtionship now gives us all coefficients in terms of $A_0$.
The first couple coefficients are,
$$
A_1 = \frac{ A_0 }{1^2} = A_0
$$

$$
A_2 = \frac{ A_1 }{2^2} = \frac{ A_0 }{2^2}
$$

$$
A_3 = \frac{ A_2 }{3^2} = \frac{ A_0 }{2^2 3^2}
$$

This means that our general solution looks like
$$
f(z) = \sum_{k=0} A_k z^{k+1}
= A_0 \sum_{k=0} \frac{ z^{k+1} }{k!^2}
$$



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Legendre Equation}

Using the nomenclature of Arfken,
$$
\mathcal{L} y(x) =
-(1-x^2) y^{\prime\prime} + 2x y^\prime = \lambda y
$$

Which can also be expressed as
$$
(1-x^2) y^{\prime\prime} - 2x y^\prime + \lambda y = 0
$$

If we assume the solution is of the form
$$
y(x) =
\sum_{j=0} a_j x^{j+s}
$$

then the Legendre equation can be written as

\begin{align*}
& (1-x^2) \sum_{j} a_j (j+s)(j+s-1) x^{j+s-2}
    - 2x \sum_{j} a_j (j+s) x^{j+s-1}
    + \lambda \sum_{j} a_j x^{j+s} \\
&= \sum_{j} a_j (j+s)(j+s-1) x^{j+s-2}
    - \sum_{j} a_j (j+s)(j+s-1) x^{j+s}
    -2 \sum_{j} a_j (j+s) x^{j+s}
    + \lambda \sum_{j} x^{j+s} \\
&= \sum_{j} a_j (j+s)(j+s-1) x^{j+s-2}
    + \sum_j \left[ -(j+s)(j+s-1) -2(j+s) + \lambda \right] a_j x^{s+j}
\end{align*}

The indicial polynomial then comes from considering the lowest power of $x$, which is the first sum
$$
a_j (j+s)(j+s-1) x^{j+s-2}
$$
which for an arbitrary $x$ when $j=0$, it becomes
$$
s(s-1) = 0
$$
whose solutions are $s=0$ and $s=1$.

In this case, the solution will be a linear combination,

$$
y(x) = y_1 (x) + y_2 (x)
= \sum_j a_j x^{j} + \sum_j b_j x^{j+1}
$$

Let's start with the $s=0$ case then.
Plugging $y_1 (x)$ back into our ODE we have

\begin{align*}
& (1-x^2) \sum_j a_j j(j-1) x^{j-2}
    - 2x \sum_j a_j j x^{j-1}
    + \lambda \sum_j a_j x^j \\
&= \sum_j a_j j(j-1) x^{j-2}
    - \sum_j a_j j(j-1) x^{j}
    - 2 \sum_j a_j j x^{j}
    + \lambda \sum_j a_j x^j \\
&= \sum_j a_j j(j-1) x^{j-2}
    + \sum_j \left[ -j(j-1) -2j + \lambda \right] a_j x^j
\end{align*}

Now, to align the powers of $x^{j}$.
First, note that the first two terms in
$\sum_j a_j j(j-1) x^{j-2}$ are $0$.
So we might as well start at $j=2$
$$
\sum_{j=2} a_j j(j-1) x^{j-2}
$$

If we shift it back to $0$, we can align the powers of $x$.
$$
\sum_{j=2} a_j j(j-1) x^{j-2} =
\sum_{j=0} a_{j+2} (j+2)(j+1) x^{j}
$$

This gives us,
$$
\sum_{j=0} a_{j+2} (j+2)(j+1) x^{j}
+ \sum_j \left[ -j(j-1) -2j + \lambda \right] a_j x^j
= 0
$$

Now, we can equate coefficients,
$$
a_{j+2} (j+2)(j+1)
    + \left[ -j(j-1) -2j + \lambda \right] a_j = 0
$$
or
$$
a_{j+2} = \frac{ j(j-1) + 2j - \lambda }{ (j+1)(j+2) } a_j
= \frac{ j(j+1) - \lambda  }{ (j+1)(j+2) } a_j
$$
\\~\\

In the $s=1$ case, if we plug in $y_2$ into our ODE,

\begin{align*}
& (1-x^2) \sum_j a_j j(j+1) x^{j-1}
    - 2x \sum_j a_j (j+1) x^{j}
    + \lambda \sum_j a_j x^{j+1} \\
&= \sum_j a_j j(j+1) x^{j-1}
    - \sum_j a_j j(j+1) x^{j+1}
    - 2 \sum_j a_j (j+1) x^{j+1}
    + \lambda \sum_j a_j x^{j+1} \\
&= \sum_j a_j j(j+1) x^{j-1}
    + \sum_j \left[ -j(j+1) -2(j+1) + \lambda \right] a_j x^{j+1}
\end{align*}

Following the procedure we have been building: look for a series with redundant terms, and then try to shift it to match
the other powers of $x$.
Here we have
$$
\sum_{j=0} a_j j(j+1) x^{j-1}
$$
that can be written as
$$
\sum_{j=1} a_j j(j+1) x^{j-1}
$$
Since the term when $j=0$ equals to 0.
And we can conveniently shift this series back one to match the $x^j$ terms of the other sum:

$$
\sum_{j=1} a_j j(j+1) x^{j-1}
=
\sum_{j=0} a_{j+1} (j+1)(j+2) x^{j}
$$

At this point we have,

$$
\sum_{j=0} a_{j+1} (j+1)(j+2) x^{j}
    + \sum_j \left[ -j(j+1) -2(j+1) + \lambda \right] a_j x^{j+1}
$$

We need to do another shift and doing it on the first sum seems like the simplest approach.
If we do so, then we can arrive at

$$
\sum_{j=-1} a_{j+2} (j+2)(j+3) x^{j+1}
    + \sum_j \left[ -j(j+1) -2(j+1) + \lambda \right] a_j x^{j+1}
$$

But let's separate the first term of the first sum.
When $j=-1$, we get $a_1 (1)(2) x^0 = 2a_1$.
Thus,

$$
2 a_1 +
    \sum_{j=0} a_{j+2} (j+2)(j+3) x^{j+1}
    + \sum_j \left[ -j(j+1) -2(j+1) + \lambda \right] a_j x^{j+1} = 0
$$

And this equation must hold for all $x$, so the coefficients of each power of $x$ must be zero.
This means that $a_1 must be 0$.
And we are left with
$$
a_{j+2} (j+2)(j+3)
    + \left( -j(j+1) -2(j+1) + \lambda \right) a_j = 0
$$

Which simplifies to
$$
a_{j+2} = \frac{ j^2 + 3j + 2 - \lambda }{ (j+2)(j+3) } a_j
= \frac{ (j+1)(j+2) - \lambda }{ (j+2)(j+3) } a_j
$$

In order to obtain the eignevalues we need to go back to the beginning.
If you remember, this is how the problem was initially framed:
$$
\mathcal{L} y(x) =
-(1-x^2) y^{\prime\prime} + 2x y^\prime = \lambda y
$$

So whatever we can do to find a $\lambda$ will give us our eigenvalues.
And that is why Arfken and other resources look for the conditions that make both of the recursion relations result in
a series solution that converges.
In the first case we need a value of lambda that will cancel the numerator $j(j+1)$ and in the second we need a value that
will cancel the $(j+1)(j+2)$.
