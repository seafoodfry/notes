\section{Integrals in QFT}

\subsection{Kinetic operator and a Green Function} \label{integral:kinetic-operator}

This problem is part of
\href{https://arxiv.org/abs/physics/0609191}{MontePython: Implementing Quantum Monte Carlo using Python}.

There is a kinetic operator defined as
$$
G_K \left(\mathbf{R}^\prime, \mathbf{R}, t\right) =
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-i\mathbf{k}\mathbf{R}^\prime} e^{-Dtk^2} e^{-i\mathbf{k}\mathbf{R}} d\mathbf{k}
$$

This integral then is integrated and the following Green function is obtained
$$
G_K \left(\mathbf{R}^\prime, \mathbf{R}, t\right) =
\frac{1}{\left(4\pi Dt\right)^{3N/2}} e^{-\left(\mathbf{R}-\mathbf{R}^\prime\right) / 4Dt}
$$

And as we saw in \hyperref[jackson:problem-1.2]{Jackson problem 1.2}, the Green function we just got is equivalent to
$\delta\left(\mathbf{R} - \mathbf{R}^\prime\right)$.

But coming back to the integral, the trick is to complete the square!
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-i\vect{k}\vect{R}^\prime} e^{-Dtk^2} e^{-i\vect{k}\vect{R}} d\vect{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dtk^2 + -i\vect{k}\cdot\left(\vect{R}-\vect{R}^\prime\right)} d\mathbf{k}
\end{align*}

This last integral has the form $\int e^{ax^2 + bx} dx$, which can be solved as follows,
\begin{align*}
\int e^{ax^2 + bx} dx &= \int \exp\left\{ a\left(x^2 + \frac{b}{a}x\right) \right\} dx \\
&= \int \exp\left\{ a\left(x^2 + \frac{b}{a}x + \left(\frac{b}{2a}\right)^2 - \left(\frac{b}{2a}\right)^2 \right) \right\} dx \\
&= \int \exp\left\{ a\left(x + \frac{b}{2a}\right)^2 - \frac{b^2}{4a} \right\} dx \\
&= e^{-b^2/4a} \int e^{ a\left(x + \frac{b}{2a}\right)^2 } dx
\end{align*}

Now, let's look at $\int e^{ a\left(x + b/2a\right)^2 } dx$.
First, we need to make a transformation.
Let's define $u = x + b/2a$, then $du = dx$, the measure remains invariant (so do the limits of integration), so
$\int e^{ a\left(x + b/2a\right)^2 } dx = \int e^{ au^2 } du$.
Now, if we define $a = -c$, then 

\begin{align*}
\int e^{ a\left(x + b/2a\right)^2 } dx &= \int e^{ au^2 } du \\
&= \int e^{ -cu^2 } du \\
&= \sqrt{ \frac{\pi}{c} } = \sqrt{ \frac{\pi}{-a} } 
\end{align*}

This whole thing works if $a < 0$ but it turns out that this is also valid if $\re{(a)} \leq 0$ but $a \neq 0$.

Putting everything back together,
$$
\int e^{ax^2 + bx} dx = \sqrt{ \frac{\pi}{-a} } e^{-b^2/4a}
$$

Looking back at our original problem, we can chose $a = -Dt$ and $b = i \vect{r}$, where $\vect{r} = \vect{R}-\vect{R}^\prime$.
and so
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dtk^2 + -i\vect{k}\cdot\vect{r}} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k^2 + \frac{i\vect{k}}{Dt}\cdot\vect{r} \right)} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k^2 + \frac{i\vect{r}}{Dt}\cdot\vect{k} + \left(\frac{i\vect{r}}{2Dt}\right)^2 - \left(\frac{i\vect{r}}{2Dt}\right)^2 \right)} d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 - \frac{ \vect{r}^2 }{ 4Dt } } d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } d\mathbf{k} \\
\end{align*}

Again, since $\int e^{-ax^2} dx = \sqrt{ \frac{\pi}{a} }$,
then
\begin{align*}
\int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } dk &= \sqrt{ \frac{\pi}{Dt} }
\end{align*}

Thus,
\begin{align*}
G_K \left(\vect{R}^\prime, \vect{R}, t\right) &=
\frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \int e^{-Dt \left( k + \frac{i\vect{r}}{2Dt}\right)^2 } d\mathbf{k} \\
&= \frac{1}{\left(2\pi\right)^{3N}} e^{-\vect{r}^2 /4Dt } \left( \sqrt{ \frac{\pi}{Dt} } \right)^{3N} \\
&= \frac{1}{\left(4\pi^2\right)^{3N/2}} \frac{\pi^{3N/2}}{(Dt)^{3N/2}} e^{-\vect{r}^2 /4Dt } \\
&= \frac{1}{\left(4\pi Dt\right)^{3N/2}} e^{-\left(\vect{R} - \vect{R}^\prime\right)^2 /4Dt }
\end{align*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Klein-gordon Non-relativistic Amplitude} \label{ps-kg:nonrelativistic-propagator}

The following integral comes from Peskin and schroeder Section 2.1
\begin{align}
& \int \frac{d^3p}{(2\pi)^3} \, e^{-i(\vect{p}^2/2m)t} e^{i\vect{p}\cdot(\vect{x}-\vect{x}_0)} \label{kg:nonrelativistic-03} \\
&= \left( \frac{m}{2\pi i t} \right)^{3/2} e^{im (\vect{x}-\vect{x}_0)^2 /2t} \label{kg:nonrelativistic-04}
\end{align}

This integral follows the same procedure as \ref{integral:kinetic-operator}.

Here we have $a = -\frac{it}{2m}$ and $b = i(\vect{x}-\vect{x_0})$.
Since $\re{a} = -t/2m \leq 0$ and $a\neq 0$ when $t\neq 0$ oru solution is valid for $t\neq 0$.
Then

$$
\sqrt{ \frac{\pi}{-a} } = \sqrt{ \frac{2\pi m}{it} }
$$

and
\begin{align*}
-\frac{b^2}{4a} &= \frac{2m \left(\vect{x}-\vect{x}_0\right)^2}{-4it} \\
&= \frac{im \left(\vect{x}-\vect{x}_0\right)^2}{2t}
\end{align*}

So the solution is
\begin{align*}
& \frac{1}{(2\pi)^3} \left( \frac{2\pi m}{it} \right)^{3/2} e^{\frac{im \left(\vect{x}-\vect{x}_0\right)^2}{2t}} \\
&= \left( \frac{m}{2\pi it} \right)^{3/2} e^{im \left(\vect{x}-\vect{x}_0\right)^2 /2t}
\end{align*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tensors}

Working with a spacetime with a metric signature $(+ - - -)$,
$$
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
$$

$x^\mu = (x^0, \vect{x})$ and $x_\mu = g_{\mu\nu} x^\nu = (x^0, -\vect{0})$.
To see this explciitly, let's carry out the summations.

When $\mu = 0$, $x_0 = g_{00}x^0 + g_{01}x^1 + g_{02}x^2 + g_{03}x^3 = x^0 + 0 + 0 +0 = x^0$.
For $\mu = 1$, $x_1 = g_{10}x^0 + g_{11}x^1 + g_{12}x^2 + g_{13}x^3 = 0 + x^1 + 0 +0 = -x^0$.
Similarly, when $\mu = 2$, $x_2 = -x^2$, and when $\mu =3$, $x_3 = -x^3$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inner Product}

Why does $p \cdot x = g_{\mu\nu} p^\mu x^\nu = g^{\mu\nu} p_\mu x_\nu = p_\mu x^\nu = p^\mu x_\nu = p^0 x^0 - \vect{p}\cdot\vect{x}$?

The metric tensor defines the geometry of spacetime, including the way distances and angles are measured.
The implied summation, einstein summation, effectively 'weights' the components according to the geometry of spacetime.

$g_{\mu\nu}$ lowers indices, while $g^{\mu\nu}$ raises them.
$\delta^{\mu}_{\nu}$ is a diagonal matrix with ones on the diagonal and zeros elsewhere.
It selects the $\mu$-th component when used in a summation, acting like an identity element.

$g^{\mu\alpha} g_{\alpha\nu} = \delta^{\mu}_{\nu}$

$$
g^{\mu\alpha} g_{\alpha\nu} = \sum_{\alpha = 0} g^{\mu\alpha} g_{\alpha\nu} =
g^{\mu 0} g_{0 \nu} + g^{\mu 1} g_{1\nu} + g^{\mu 2} g_{2\nu} + g^{\mu 3} g_{3\nu}
$$

This is essentially a matrix multiplication.
$$
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
\cdot
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
=
\begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
\end{pmatrix}
$$

Going component by component, take the second row and second column, $\mu=1$ and $\nu=1$.
The element for that entry is given by
$$
g^{1\alpha}g_{\alpha 1} =
g^{1 0} g_{0 1} + g^{1 1} g_{1 1} + g^{1 2} g_{2 1} + g^{1 3} g_{3 1}
= 0 + (-1)(-1) + 0 + 0 = 1
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Tensors}

Working with a spacetime metric signature $(+ - - -)$,
$$
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
$$

$x^\mu = (x^0, \vect{x})$ and $x_\mu = g_{\mu\nu} x^\nu = (x^0, -\vect{x})$.
To see this explciitly, let's carry out the summations.

When $\mu = 0$, $x_0 = g_{00}x^0 + g_{01}x^1 + g_{02}x^2 + g_{03}x^3 = x^0 + 0 + 0 +0 = x^0$.
For $\mu = 1$, $x_1 = g_{10}x^0 + g_{11}x^1 + g_{12}x^2 + g_{13}x^3 = 0 + x^1 + 0 +0 = -x^0$.
Similarly, when $\mu = 2$, $x_2 = -x^2$, and when $\mu =3$, $x_3 = -x^3$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inner Product}

Why does $p \cdot x = g_{\mu\nu} p^\mu x^\nu = g^{\mu\nu} p_\mu x_\nu = p_\mu x^\nu = p^\mu x_\nu = p^0 x^0 - \vect{p}\cdot\vect{x}$?

The metric tensor defines the geometry of spacetime, including the way distances and angles are measured.
The implied summation, einstein summation, effectively 'weights' the components according to the geometry of spacetime.

$g_{\mu\nu}$ lowers indices, while $g^{\mu\nu}$ raises them.
$\delta^{\mu}_{\nu}$ is a diagonal matrix with ones on the diagonal and zeros elsewhere.
It selects the $\mu$-th component when used in a summation, acting like an identity element.

$g^{\mu\alpha} g_{\alpha\nu} = \delta^{\mu}_{\nu}$

$$
g^{\mu\alpha} g_{\alpha\nu} = \sum_{\alpha = 0} g^{\mu\alpha} g_{\alpha\nu} =
g^{\mu 0} g_{0 \nu} + g^{\mu 1} g_{1\nu} + g^{\mu 2} g_{2\nu} + g^{\mu 3} g_{3\nu}
$$

This is essentially a matrix multiplication.
$$
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
\cdot
\begin{pmatrix}
    1 & 0  & 0  & 0  \\
    0 & -1 & 0  & 0  \\
    0 & 0  & -1 & 0  \\
    0 & 0  & 0  & -1 \\
\end{pmatrix}
=
\begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1 \\
\end{pmatrix}
$$

Going component by component, take the second row and second column, $\mu=1$ and $\nu=1$.
The element for that entry is given by
$$
g^{1\alpha}g_{\alpha 1} =
g^{1 0} g_{0 1} + g^{1 1} g_{1 1} + g^{1 2} g_{2 1} + g^{1 3} g_{3 1}
= 0 + (-1)(-1) + 0 + 0 = 1
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Derivatives}

The examples here come from \cite{tensor-calc}.
The aim here is to see a few examples to get used to the quantities we are going to be seeing later on.
\\

Some useful identities before we carry on:
\begin{enumerate}
    \item
    $$
    \frac{\partial x_p}{\partial x_q} = \delta_{pq}
    $$
\end{enumerate}



\textbf{Example 1}

If $a_{ij}$ are contants, to compute the partial derivative
$$
\frac{\partial}{\partial x_k} a_{ij} x_i x_j
$$

We'll firs write out the sums explciitly.
\begin{align*}
a_{ij} x_i x_j &= \sum_{i,j} a_{ij} x_i x_j
\end{align*}

Someone once happened to noticed that the sum could be broken into parts depending on when $x_{\{i,j\}} = x_k$, where $x_k$
is the one we want to differentiate with respect to,
\begin{align*}
\sum_{i,j} a_{ij} x_i x_j &=
    \sum_{i \neq k, j \neq k} a_{ij} x_i x_j +
    \sum_{i, j \neq k} a_{ij} x_i x_j + 
    \sum_{i \neq k, j} a_{ij} x_i x_j +
    \sum_{i=k, j=k} a_{ij} x_i x_j \\
&= C + \sum_{j \neq k} \left( a_{kj} x_j \right) x_k +
    \sum_{i \neq k} \left( a_{ik} x_i \right) x_k +
    a_{kk} x_k x_k \\
    &= C + \sum_{j \neq k} \left( a_{kj} x_j \right) x_k +
    \sum_{i \neq k} \left( a_{ik} x_i \right) x_k +
    a_{kk} (x_k)^2 \\
\end{align*}

We wrote the first sum, where no $x$s are equal to $x_k$ as $C$, because as you may guess this is going to go away with
any derivative.

Back to our original task,
\begin{align*}
\frac{\partial}{\partial x_k} a_{ij} x_i x_j &=
    0 + \sum_{j \neq k} a_{kj} x_j +
    \sum_{i \neq k} a_{ik} x_i +
    2 a_{kk} x_k \\
&= \sum_{j} a_{kj} x_j + \sum_{i} a_{ik} x_i \\
&= a_{kj} x_j + a_{ik} x_i \\
&= a_{ki} x_i + a_{ik} x_i \\
&= (a_{ik} + a_{ki}) x_i
\end{align*}



\textbf{Example 2}

Another way to work out
$$
\frac{\partial}{\partial x_k} a_{ij} x_i x_j
$$
goes as follows.

\begin{align*}
\frac{\partial}{\partial x_k} a_{ij} x_i x_j &= a_{ij} \frac{\partial}{\partial x_k} x_i x_j \\
&= a_{ij} \left( \frac{\partial x_i}{\partial x_k} x_j + x_i \frac{\partial x_j}{\partial x_k} \right) \\
&= a_{ij} \left( \delta_{ik} x_j + x_i \delta_{jk} \right) \\
&= a_{kj} x_j + a_{ik} x_i \\
&= (a_{ik} + a_{ki}) x_i
\end{align*}


\textbf{Example 3}

If $a_{ij} = a_{ji}$ are constants, compute
$$
\frac{\partial^2}{\partial x_k \partial x_l} a_{ij} x_i x_j
$$

Let's start with our previous result,
\begin{align*}
\frac{\partial^2}{\partial x_k \partial x_l} a_{ij} x_i x_j &=
    \frac{\partial}{\partial x_k} \left[ \frac{\partial}{\partial x_l} a_{ij} x_i x_j \right] \\
&= \frac{\partial}{\partial x_k} (a_{il} + a_{li}) x_i \\
&= 2 a_{li} \frac{\partial x_i}{\partial x_k} \\
&= 2 a_{li} \delta_{ik} \\
&= 2 a_{lk}
\end{align*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Changing Variables: the Jacobian}


Promise we are not doing the conventional thing where we go from scratch but there is one more pattern we have to covered
now in order for some other equations to make sense and this is the Jacobian.


There is a result from calculus that is presented a bit like this (the following presentation comes from
\cite{jacobian}):


If we define the Jacobian of a transformation $x = g(u,v)$, $y = h(u,v)$ as
\begin{align*}
\frac{ \partial (x,y) }{ \partial (u,v) } &= 
\begin{vmatrix}
\dfrac{ \partial x }{ \partial u } & \dfrac{ \partial x }{ \partial v } \\ \addlinespace
\dfrac{ \partial y }{ \partial u } & \dfrac{ \partial y }{ \partial v }
\end{vmatrix} \\
&= \frac{ \partial x }{ \partial u } \frac{ \partial y }{ \partial v } -
    \frac{ \partial x }{ \partial v }\frac{ \partial y }{ \partial u }
\end{align*}

Then if we want to integrate $f(x)$ over some surface $R$, this region will become some other surface $S$
following our transformation and the integral can be computed as follows:
$$
\int \int_{R} f(x,y) dA
= \int \int_{S} f\left( g(u,v), h(u,v) \right) \Biggl| \frac{ \partial (x,y) }{ \partial (u,v) } \Biggl| d\overline{A}
$$

For example, when we go from Euclidean to polar coordinates, the transformation we use is $x = r\cos\theta$
and $y = r\sin\theta$ and the Jacobian is $r$ so
$dA = \left| \frac{\partial (x, y)}{\partial (r, \theta)} \right| dx dy = r dr d\theta$:

\begin{align*}
\frac{\partial (x, y)}{\partial (r, \theta)} &= 
\begin{vmatrix}
    \dfrac{ \partial x }{ \partial r } & \dfrac{ \partial x }{ \partial \theta } \\ \addlinespace
    \dfrac{ \partial y }{ \partial r } & \dfrac{ \partial y }{ \partial \theta }
\end{vmatrix} \\
& = \begin{vmatrix}
    \cos\theta & -r\sin\theta \\ \addlinespace
    \sin\theta & r\cos\theta
\end{vmatrix} \\
&= r \cos^2 \theta + r \sin^2 \theta \\
&= r
\end{align*}



In 3-dimensions this extends to
$$
\int \int \int_{V} f(x,y,z) dV
= \int \int \int_{B} f\left( g(u,v,w), h(u,v,w), k(u,v,w) \right) \Biggl| \frac{ \partial (x,y,z) }{ \partial (u,v,w) } \Biggl| d\overline{V}
$$

For the case of the transformation of Euclidean coordinates to spherical coordinates, the transformation of variables is
$x = r \sin\phi \cos\theta $, $y = r \sin\phi \sin\theta$, and $z = r\cos\phi$.
In that case the Jacobian is $r^2 \sin\phi$ and thus
$dV = \left| \frac{\partial (x, y, z)}{\partial (r, \theta, \phi)} \right| dx dy dz = r^2 \sin\phi dr d\phi d\theta$.


Besides being very widely used, the reason we mentioned it is because this is also that is very well used in
\cite{gifted-qft} to introduce the notation we see so much in QFT.
Specifically, the Jacobian as we have been writing it, can also be seen in expressions such as

$$
\overline{a}^{\mu} = \left( \frac{\partial \overline{x}^\mu}{\partial x^\nu} \right) a^\nu
$$

$$
\frac{\partial \phi}{\partial \overline{x}^\mu} =
    \left( \frac{\partial x^\nu}{\partial \overline{x}^\mu} \right) \frac{\partial \phi}{\partial x^\nu}
$$

The determinant of the Jacobian matrix is used specifically when you are dealing with volume transformations
(like integrating over a volume or changing the measure of integration) because it scales the volume elements according
to how the coordinate transformation stretches or compresses the space.

When transforming vectors, however, you're not scaling a volume but rather reorienting or rescaling each component
of the vector according to how the coordinate axes themselves have changed.
This is why the transformation involves a sum over the product of vector components and the appropriate Jacobian elements:

\begin{itemize}
\item Contravariant vectors (standard vectors): The components of these vectors are transformed by multiplying with the
    Jacobian matrix $\partial \overline{x}^\mu / \partial x^\nu$. Here, each new vector component is a linear combination
    of old components weighted by how much each new coordinate axis changes with respect to each old axis.
    \begin{itemize}
        \item These objects, such as displacement vectors, inherently "follow" the coordinate axes. As the axes stretch
        or rotate, so do the components of the vector. 
    \end{itemize}
\item Covariant vectors (gradients and one-forms): These vectors transform using the inverse of the Jacobian matrix
    $\partial x^\nu / \partial \overline{x}^\mu$. This reflects how changes in the old coordinates map to the new
    coordinates, suitable for objects that naturally pair with vectors, like gradients.
    \begin{itemize}
        \item They measure rates of change along these coordinates.
        \item A larger coordinate stretch means a smaller gradient in that direction to maintain the same rate of change.
    \end{itemize}
\end{itemize}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Pauli Matrices}

The Pauli matrices actually form a basis for $2 \times 2$ Hermitian matrices.
Let's see how: a generic $2 \times 2$ Hermitian matrix would look like

$$
\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}
=
\begin{pmatrix}
a^* & c^* \\
b^* & d^*
\end{pmatrix}
$$

Meaning that $a, b \in \mathbb{R}$ and that $b = c^*$.
So we could write our Hermitian matrix as

$$
\begin{pmatrix}
a      & x - iy \\
x + iy & d
\end{pmatrix}
$$

Another requirement that is imposed on $H_2 (\mathbb{C})$ is that they must be unitary, that is $U^\dagger U = I$, so

\begin{align*}
\begin{pmatrix}
a      & x - iy \\
x + iy & d
\end{pmatrix}
\begin{pmatrix}
a      & x - iy \\
x + iy & d
\end{pmatrix}
&=
\begin{pmatrix}
a^2 + x^2 - y^2         & a(x - iy) + b(x - iy) \\
a(x + iy) + b(x + iy) & d^2 + x^2 - y^2
\end{pmatrix}
\\
&=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\end{align*}

One way to make this happen is to set $a = -d$, this will also have the happy side effect of meeting another requirement
for the Pauli matrices to be traceless.
So now we have our candidate bases that ought to look like

$$
\begin{pmatrix}
a      & x - iy \\
x + iy & -a
\end{pmatrix}
$$

This then means that our requirement for our matrix $H_2 (\mathbb{C})$ to be unitary simplifies to
$$
\begin{pmatrix}
a^2 + x^2 - y^2 & 0                \\
0               & a^2 + x^2 - y^2
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
$$

Here is where our 3 Pauli matrices comes!

$$
\sigma_1 = \sigma_x =  
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
$$
comes from assuming that $x=1$, so $a^2 + x^2 - y^2 = 1$ can be met by assuming that $a = y = 0$.

$$
\sigma_2 = \sigma_y =  
\begin{pmatrix}
0 & -i \\
i & 0
\end{pmatrix}
$$

comes from assuming that $y=1$, so $a^2 + x^2 - y^2 = 1$ can be met by assuming that $a = x = 0$.

Finally,
$$
\sigma_3 = \sigma_z =  
\begin{pmatrix}
1 & 0 \\
0 & -1
\end{pmatrix}
$$

Where we took $a=1$ and $x = y = 0$.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{From Tensors to Matrices}

tensors can be expressed as matrices only once a basis has been chosen.
the example being given is with the angular momentum operator $L_z$ on the spherical harmonics $H_l (\mathbb{R}^3)$.

It says that $H_l (\mathbb{R}^3)$ is the set of all linear functions on $\mathbb{R}^3$ that

$$
\{ r Y_{m}^{l} \}_{ -1 \leq m \leq 1 } =
\Big\{ \frac{1}{\sqrt{2}} (x +iy), \, z, \, - \frac{1}{\sqrt{2}} (x -iy) \Big\}
$$

and $\{ x, y, z \}$ are both basis for the space.

Then having the angular momentum operator $L_z = -i ( x\partial_y - y \partial_x )$ on this space we got
$$
\frac{1}{\sqrt{2}} L_z (x + iy) = \frac{1}{\sqrt{2}} (x + iy)
$$
$$
L_z (z) = 0
$$
$$
-\frac{1}{\sqrt{2}} L_z (x -iy) = \frac{1}{\sqrt{2}} (x -iy)
$$

which implies that in the spherical harmonics basis

$$
L_z = 
\begin{pmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & -1
\end{pmatrix}
$$

Could you tell me how that tensor representation of $L_z$ is implies by saying that
$T(e_i) = \sum^{n}_{j=1} T_{i}^{j} e_j$ ?


$$
\frac{1}{\sqrt{2}} L_z (x + iy)
\rightarrow
L_z \left( \frac{1}{\sqrt{2}} (x + iy) \right)
$$

$$
L_z (z) = 0
$$

$$
-\frac{1}{\sqrt{2}} L_z (x -iy)
\rightarrow
L_z \left( -\frac{1}{\sqrt{2}} (x + iy) \right)
$$

Since

$$
\{ r Y_{m}^{l} \}_{ -1 \leq m \leq 1 } =
\Big\{ \frac{1}{\sqrt{2}} (x +iy), \, z, \, - \frac{1}{\sqrt{2}} (x -iy) \Big\}
$$

then we can say that $e_1 = \frac{1}{\sqrt{2}} (x +iy)$, $e_2 = z$, and $e_3 = -\frac{1}{\sqrt{2}} (x -iy)$.

Meaning that $L_z (e_1) = e_1 = 1\cdot e_1 + 0\cdot e_2 + 0\cdot e_3$,
$L_z (e_2) = 0 = 0\cdot e_1 + 0\cdot e_2 + 0\cdot e_3$, and
$l_z (e_3) = -e_3 = 0\cdot e_1 + 0\cdot e_2 - 1\cdot e_3$.
From here, we can then adopt write this as a matrix

$$
T_{i}^{j}
=
L_z
=
\begin{pmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & -1
\end{pmatrix}
$$

The columns of a matrix representation of a linear operator correspond to the action of the operator on the basis vectors.
However, due to the specific diagonal form of the matrix in this example, both the row and column perspectives
give the same result, which can indeed be confusing.

The first column represents $L_z(e_1)$,
the second column represents $L_z(e_2)$,
the third column represents $L_z(e_3)$.

In physics speak this would be read as the wave functions
$\frac{1}{\sqrt{2}} (x +iy)$, $z$, and $- \frac{1}{\sqrt{2}} (x -iy)$
have eigenvalues 1, 0, and -1, respectively.